[0:00:00] hello guys welcome to my new video on how to  code stable diffusion from scratch and stable  
[0:00:06] diffusion is a model that was introduced last year  I think most of you are already familiar with it  
[0:00:12] and we will be coding it from scratch using  pytorch only and as usual my video is going  
[0:00:18] to be quite long and because we will be calling  coding from scratch and at the same time I will  
[0:00:23] be explaining each part that makes up stable  diffusion so as usual let me introduce you what  
[0:00:29] the what are the topics that we will discuss and  what are the prerequisites for watching this video  
[0:00:35] so of course we will discuss the stable  diffusion because we are going to build  
[0:00:39] it from scratch using only python so no other  libraries will be used except for the tokenizer  
[0:00:45] I will describe the maths of the division models  as defined in the ddpm paper but I will simplify  
[0:00:51] it as a simplify it as much as possible I will  show you how classifier free guidance work as  
[0:00:56] and we of course we will also implement it how the  text to image Works image to image and in painting  
[0:01:03] of course to have a very complete view of  diffusion model so actually we should also  
[0:01:07] introduce the score based models and all the ode  and SDF theoretical framework but most people  
[0:01:13] are not are not familiar with the ordinary  differential equations or even stochastic  
[0:01:17] differential equations so I will not discuss  these topics in this video and I'll leave it  
[0:01:22] for future videos so anyway we will have a  complete copy of a stable diffusion with the  
[0:01:28] uh we will be able to do uh generate images using  the prompt also condition on existing images  
[0:01:34] Etc but for example the Samplers based  on the earlier method or runjikuta method  
[0:01:40] will not be built in this video I will make a  future video in which I describe this one so  
[0:01:46] what do I expect you to have as a prerequisite for  watching this video well first of all it's good  
[0:01:52] that if you have some notion of probability and  statistics so at least you know what is a gaussian  
[0:01:57] distribution what is the conditional probability  the marginal probability the likelihood Etc now  
[0:02:03] I don't expect you to have the mathematical  formulation in your mind about these Concepts  
[0:02:08] but at least the concepts behind them so at least  what do we mean by conditional probability or what  
[0:02:13] do we mean by marginal probability anyway even if  you are not very strong with mathematics I will  
[0:02:18] always give a non-mathematics intuition for most  Concepts so even if you don't have this background  
[0:02:23] you will at least understand the concept behind  this some intuition behind it is and of course I  
[0:02:32] expect you to know Python and pytorch at least  a basic level because we will be coding using  
[0:02:38] Python and pytorch and then we will be using  a lot the attention mechanism so if you're not  
[0:02:44] familiar with the Transformer model please watch  my previous video on that uh on the attention and  
[0:02:48] Transformer and we will be also be using a lot  of convolutions so I don't expect you to know how  
[0:02:55] mathematical the convolution layers work but at  least what they what they do on a practical level  
[0:03:00] in a neural network anyway I will also review  this while coding and because this is going to  
[0:03:08] be a long video I will first because the stable  diffusion and the division models in general are  
[0:03:14] quite complex from a mathematical point of view  so we cannot jump directly to the code without  
[0:03:19] explaining what we are going to code and how  it works the first thing I will do is to give  
[0:03:25] you some background knowledge from a mathematical  point of view but also from a conceptual point of  
[0:03:30] view of how this the division models work and how  stable efficiency stable division works and then  
[0:03:37] we will build each part uh one by one of course  at the beginning you will have a lot of ideas  
[0:03:44] that are kind of confused because I will give  you a lot of new Concepts to to grasp and it's  
[0:03:50] normal that you don't understand everything at the  beginning but don't worry because while coding I  
[0:03:55] will repeat each concept more than once so while  coding you will also get a practical knowledge of  
[0:04:01] what each part is doing and how they interact with  each other so please don't be scared if you don't  
[0:04:07] understand everything in the beginning part of  this video later when we start coding it you will  
[0:04:13] everything will make sense to you but we need this  initial part because otherwise we cannot just jump  
[0:04:18] in the dark and start coding without knowing what  we are going to code so let's start our journey  
[0:04:25] uh so what is a stable diffusion stable diffusion  is a model that was introduced in 2022 so last  
[0:04:33] year at the end of last year I Remember by  a conflicts group at the Ludwig Maximilian  
[0:04:38] University in Munich Germany and it's open source  the weights the pre-trained rates can be found on  
[0:04:44] the internet and it became very famous because  people started doing a lot of stuff and building  
[0:04:49] projects with them and products with them with  the stable diffusion and one of the most simple  
[0:04:55] simple use of the stable diffusion is to do  a text to image so given a prompt we want to  
[0:04:59] generate an image we will also see how image to  image works and also how impainting Works image  
[0:05:06] to image means that you already have a picture  for example of a dog and you want to change it a  
[0:05:11] little bit by using a prompt for example you want  to ask the model to add the wings to the dock so  
[0:05:16] that it looks like a flying dog or impending means  that you remove some part of the image for example  
[0:05:22] you can remove I don't know this part here and  you ask the model to replace it with some other  
[0:05:28] parts that makes sense that is coherent with  the image and we will see also how this works  
[0:05:35] let's jump into uh generative models because  division models are generative models but  
[0:05:42] what is a generative model well a generative model  learns a probability distribution of the data such  
[0:05:49] that we can then sample from the distribution to  create new instances of the data for example if we  
[0:05:56] have many pictures of cats or dogs or whatever we  have we can train a generator generative model on  
[0:06:02] it and then we can sample from this distribution  to create new images of cats or dogs or whatever  
[0:06:08] and this is exactly what we do with stable  diffusion we actually have a lot of images  
[0:06:12] we train it on a massive amount of images and then  we sample from this distribution to generate new  
[0:06:18] images that don't exist in our training set  but the question may arise in your mind is  
[0:06:25] why do we model data as distributions as  probability distributions well let me give  
[0:06:31] you an example imagine you are a criminal and  you want to generate thousands of fake identities  
[0:06:38] imagine we also live in a very simple world  and each fake identity is made up of variables  
[0:06:43] representing the characteristic of a person so  age and height suppose we only have two variables  
[0:06:48] that make up a person so it's the age of the  person and the height of the person in my case  
[0:06:54] I will be use the centimeter for the height  I think the Americans can convert it to feet  
[0:06:59] and um so how do we proceed if you are a  criminal with this goal well we can ask  
[0:07:05] the statistic Department of the government to  give us some statistics about the age and the  
[0:07:09] height of the population this information you can  easily find online for example and um and then we  
[0:07:16] can sample from this distribution for example  if we model the age of the population like a  
[0:07:22] gaushan with the mean of 40 and the variance  of 30 okay these numbers are made up I don't  
[0:07:27] know if they reflect the reality and the height in  centimeters is 120 as mean and the variance is 100  
[0:07:37] um we get these two distributions then we  can sample from this do two distributions  
[0:07:43] to generate a fake identity what does it mean  to sample from a distribution to sample from  
[0:07:49] this kind of distribution means to throw  a coin a very special coin that has a very  
[0:07:54] high chance of falling in this area a lower  chance of falling in this area an even lower  
[0:08:01] chance of falling in this area and a very  nearly zero chance of falling in this area  
[0:08:06] so imagine we flip this coin once for the age  for example and it falls here so it's quite  
[0:08:14] probable not very probable but quite probable  so suppose the age is a tree and let me write
[0:08:23] so the age let's say is
[0:08:27] 3 and then we um  
[0:08:30] toss again this coin and we and the coin Falls  let's say here so 100 let's say 30. height
[0:08:41] 130 centimeter so as you can see the combination  of age and height is quite improbable in reality  
[0:08:49] I mean not three years old is one meter and 30  centimeters high I mean at least not the ones  
[0:08:56] I know so this combination of age height is very  um not plausible so to produce plausible pairs so  
[0:09:04] we actually need to model these two variables so  the age and height not as independent variables  
[0:09:10] and samples from each of them independently  but as a joint distribution and usually we  
[0:09:16] represent the joint distribution like this  where each combination of age and height has  
[0:09:22] a probability score associated with it and from  this distribution we only sample using one coin  
[0:09:28] and for example this coin will have a very high  probability with very high chance will fall in  
[0:09:34] this area with less chance will fall in this  area and very close to zero chance of falling  
[0:09:39] in this area suppose we throw coin and it ends  up in this area to get the corresponding suppose  
[0:09:45] this is the age and this is the height we to get  the corresponding agent height we just need to  
[0:09:51] do like this and suppose these are actually the  real height and the real height now the numbers  
[0:09:55] here are actually do not match but you got the  idea that we to model something we need a joint  
[0:10:02] distribution over all the variables and this  is actually what we do also with our images we  
[0:10:07] don't it's a with our images we create a very  complex distribution in which for example each  
[0:10:13] pixel is a distribution and the entirety of all  the pixels are one big joint distribution and  
[0:10:21] once we have a jointed distribution we can do  a lot of interesting things for example we can  
[0:10:25] marginalize so for example imagine we have a joint  distribution over the age and the height so let's  
[0:10:31] call the age X and let's call the height let's  say y so if we have a joint distribution which  
[0:10:38] means having a p of X and Y which is defined  for each combination of X and Y we can always  
[0:10:47] calculate P of X so the probability of over the  single variable by marginalizing over the other so  
[0:10:53] as the integral of P of X and Y and d y and this  is how we marginalize which means marginalizing  
[0:11:03] over all the possible y that we can have and  then we can also calculate the probability the  
[0:11:09] conditional probability for example we can say  that the probability what is the probability of  
[0:11:14] the age being let's say from 0 to 3 given that  the height is more than one meter so something  
[0:11:24] like this we can do this kind of queries by using  the conditional probability so this is actually  
[0:11:29] what we do with the generative model we model  our data as a big very big joint distribution  
[0:11:36] and then we learn the parameters of this  distribution because it's very complex  
[0:11:41] distribution so we let the natural Network  learn the parameters of this distribution  
[0:11:45] and our goal of course is to learn this very  complex distribution and then sample from it  
[0:11:52] to generate new data just like the criminal  before wanted to generate new fake identities  
[0:11:58] by modeling the very complex distribution  that represents the identity of a person  
[0:12:04] in our case we will model our system as a  joint distribution by including also some  
[0:12:10] latent variables so let me describe in as you  probably are familiar with the division models  
[0:12:16] we have two processes one is called the forward  process and one is called the reverse process  
[0:12:21] the forward process means that we have our initial  image that we will call x0 so this here and we add  
[0:12:29] the noise to it to get another image that is same  as the the previous one but with some noise on top  
[0:12:37] of it then we take this image which has a little  noise and we generate a new image that is same as  
[0:12:44] the previous one but with even more noise so as  you can see this one has even more noise and so  
[0:12:49] on so on so on until we arrive to the last latent  variable called ZT where T is equal to 1000 when  
[0:12:58] it becomes completely noise Pure Noise like n01  actually n0 I because we are in the multivariate  
[0:13:09] world and our goal actually is to this process  this forward process is fixed so we Define how  
[0:13:18] to build the noisified version of each image given  the previous one so we know how to add noise and  
[0:13:24] we have a specific formula and analytical formula  how on how to add a noise to an image the problem  
[0:13:31] is we don't have the analytical formula to reverse  this processes so we don't know how to take this  
[0:13:38] one and just remove noise there is not a there  is no closed formula on how to do it so we learn  
[0:13:45] and we team we train a neural network to do this  inverse process to remove noise from something  
[0:13:51] that has noise and if you think about it it  is quite easy to add noise to something than  
[0:13:57] it is to remove noise from something that's why  we are using a neural network for this purpose  
[0:14:04] now we need to go inside of course of the mat  because we will be using it not only to write  
[0:14:09] the code but also to write the sampler and in  the sampler it's all about mathematics and I  
[0:14:14] will try to simplify it as much as possible so  don't be scared so let's start okay this is uh  
[0:14:21] from the ddpm paper so the noising diffusion  probabilistic models from ho in 2020 and here  
[0:14:28] we have two processes the first is the forward  process which means that given the original  
[0:14:35] image how can I generally generate the noisified  version of this image at time step T in this case  
[0:14:43] actually it's a this is the joint distribution um  let's look at this one here this means if I have  
[0:14:51] the image at time step T minus 1 how can I get  the next time step so the more noisified version  
[0:14:57] of this image well we Define it as a gaussian  distribution centered so the mean centered on  
[0:15:05] the previous one and the variance defined by  this beta parameter here this beta parameter  
[0:15:12] here is decided by us and it means how much noise  we want to add at every step of this notification  
[0:15:19] process this is also known as the Markov chain of  notification because each variable is conditioned  
[0:15:26] on the previous one so to get x t we need to have  x t minus 1 and as you can see from here we start  
[0:15:33] from some x 0 we go to X1 here I call it Z1 to  differentiate it but X1 actually is equal to Z1  
[0:15:41] so x0 is the original image and all the next X are  noisy versions with x T being the most noisy so  
[0:15:50] this is called the Markov chain of nullification  and we can do it like this so it's defined by us  
[0:15:58] as a process which is a series of gaussians that  adenoid them there is an interesting formula here  
[0:16:05] this is a closed loop closed formula to go from  the original image to any image at time step T  
[0:16:13] without calculating all the intermediate images  using this particular parametrization so we can  
[0:16:21] go from the image original image to the image at  time step T by by sampling from this distribution  
[0:16:29] by defining the distribution like this so with  this mean and with this variance this mean here  
[0:16:36] depends on a parameter alpha alpha bar which is  actually depending on beta so it's something that  
[0:16:41] we know there is nothing we have to learn and also  the variance actually depends on Alpha which is  
[0:16:47] defined as in function of beta so beta is also  something we know so there is no parameters to  
[0:16:52] learn here now let's look at the reverse process  the reverse process means that we have something  
[0:16:58] noisy and we want to get something less noisy so  we want to remove noise and we also Define it as  
[0:17:05] a gaussian with a mean mu Theta and variance Sigma  Theta now the this mean and this variance are not  
[0:17:16] known to us we have to learn them and we will use  a neural network to learn these two parameters  
[0:17:22] actually the variance we will also satis at fixed  we will parameterize it in such a way that this  
[0:17:30] variance actually is fixed so we hypothesize we  already know the variance and we let the network  
[0:17:36] learn only the mean of this distribution so to  rehearse we have a forward process that adds  
[0:17:42] noise and we know everything about this process  we know how to analyze we have a reverse process  
[0:17:47] that we don't know how to denoise so we let a  network learn the parameters on how to analyze it  
[0:17:54] and okay now that we have defined these two  process how do we actually uh train a model  
[0:18:01] to do it because as you remember our initial goal  is actually to learn a probability distribution  
[0:18:06] over all over our data set and so this quantity  here but unlike before when we could marginalize  
[0:18:15] for example in the in the in the case of the  Criminal Who want to generate identities we could  
[0:18:20] marginalize over all the variables here we cannot  marginalize because we need to marginalize over  
[0:18:26] X1 X2 x t x 4 up to XT so over a lot of variables  and to calculate this integral means to call to do  
[0:18:33] calculate it over all the possible X1 and over  all the possible X2 Etc so it's a very complex  
[0:18:40] calculation that is computationally intractable  we say it means that it's a theoretically possible  
[0:18:46] but practically it will take forever so we cannot  use this root here so what can we do we want to  
[0:18:54] learn this quantity here so we want to learn the  parameter Theta of this to maximize the likelihood  
[0:18:59] we can see here what we did is we can Define we  we found a lower Bound for this quantity here so  
[0:19:07] the quantity the likelihood and this lower bound  is called the elbow and if we maximize the lower  
[0:19:13] bound it will also maximize the likelihood so let  me give you a parallel example on what it means to  
[0:19:19] maximize the lower Bound for example imagine you  have a company and your company has some Revenue  
[0:19:28] and usually the revenue is more than or equal to  the sales of your company so you have some Revenue  
[0:19:35] coming from sales maybe you also have some Revenue  coming from interest that you get from your bank  
[0:19:40] Etc but we can for sure say that the revenue of  your company is more than or equal to the sales  
[0:19:46] of your company so if you want to maximize  your Revenue you can maximize your sales for  
[0:19:52] example which is a lower bound over your uh  Revenue so if we maximize the sales we will  
[0:19:57] also maximize the revenue and this is the idea  here but how do we do it on a practical level  
[0:20:03] well this is the training code for the ddpm  diffusion models as defined by the ddpm paper  
[0:20:13] and basically the idea is after we get to the  Elbow we can parameterize it the loss function  
[0:20:19] as this which says that we need to learn we  need to train a network called Epsilon Theta  
[0:20:27] that given a noisy image so this formula  here means the noisy image at time step T  
[0:20:34] and the time Step at which the noise was added  the network has to predict how much noise is  
[0:20:40] in the image the noisified image and if we do  gradient descent over this loss function here  
[0:20:50] we will maximize the elbow and at the same time we  will also maximize the log likelihood of our data  
[0:20:59] and this is how we train this kind of networks  now I know that this is a lot of concept that  
[0:21:06] you have to grasp so don't worry for now just  remember that there is a forward process and  
[0:21:10] there is a reverse process and to train this  network to do the reverse process we need to  
[0:21:15] train a network to detect how much noise is in  a noisified version of the image at time step t
[0:21:24] um let me show you how do we once we have this  Network that has already been trained how do we  
[0:21:30] actually sample to generate new data so let's go  here let's go here so how do we generate new data  
[0:21:39] uh suppose we already have a network that was  trained for uh detecting how much noise is in  
[0:21:44] there and what we do is we start from complete  noise and then we ask the network to detect how  
[0:21:50] much noise is in there we remove this noise and  then we ask the network again how much noise is  
[0:21:56] in there and we remove it and then we ask the  network how much noise is there okay remove it  
[0:22:01] then how much noise is here okay remove it etc etc  until we reach this step then here we will have  
[0:22:08] something new so if we start from Pure Noise and  we do this reverse process many times we will end  
[0:22:14] up with something new and this is the idea behind  this generative model now that we know how to  
[0:22:20] generate new data starting from Pure Noise we also  want to be able to con control this denosification  
[0:22:28] process so we can generate images of something  that we want I mean how can we tell the model  
[0:22:34] to generate a picture of a cat or a picture of  a dog or a picture of a house by starting from  
[0:22:40] Pure Noise because as of now by starting from  Pure Noise and going uh keep denoising we will  
[0:22:47] generate a new image of course but it's not  like we can control which New Image will be  
[0:22:51] generated so we need to find a way to tell the  model what we want in this generational process  
[0:22:58] and the idea is that we start from Pure Noise  and during this chain of removing noise so  
[0:23:06] the noiseification we introduce a signal let's  call it prompt from or can also be called the  
[0:23:15] conditioning signal or it can also be called  the context anyway they are the same concept  
[0:23:21] in which we influence the model into how to  remove the noise so that the output will move  
[0:23:28] towards what we want to understand how this  works let's review again how the training of  
[0:23:34] this kind of networks works because this  is very important for us to learn how  
[0:23:39] the training of this kind of network rules so  that we can introduce the prompt let's go back
[0:23:46] okay I told you before our final goal is to  model a distribution Theta P of theta such  
[0:23:55] that we maximize the likelihood of our data and  to learn this distribution we maximize the elbow  
[0:24:05] so the lower bound but how do we maximize the  elbow we minimize this loss minimize this loss  
[0:24:14] here so by minimizing the lessness loss we  maximize the elbow which in turns learns this  
[0:24:20] distribution here because it's the this elbow  here is the lower Bound for the likelihood of  
[0:24:27] our the digital distribution here and what is  this loss function loss function here indicates  
[0:24:34] that we need to create a model Epsilon data such  that if we give this a Model A noisified image at  
[0:24:42] a particular noise level and we also tell him  what noise level we included in this image the  
[0:24:49] network has to predict how much noise is there  so this Epsilon is how much noise we have added  
[0:24:55] and this is and we can do a gradient descent  on this training Loop this way we will learn  
[0:25:02] a distribution over our data but as you can see  this distribution doesn't include anything that  
[0:25:10] tells the model what is a cat or what is a dog or  what is a house the model model is just learning  
[0:25:16] how to generate pictures that makes sense that  are similar to our initial training data but they  
[0:25:21] don't know what is the relationship between that  picture and the prompt so one idea could be okay  
[0:25:27] can we learn a distribution or joint distribution  over the our initial data so all the images and  
[0:25:35] the conditioning signal so the prompt well this  is also something that we don't want because we  
[0:25:40] want to actually learn this distribution so  that we can sample and generate new data we  
[0:25:45] don't want to learn the joint distribution that  will be too much influenced by the context and  
[0:25:50] the model may not learn the generative process of  the data so our final goal is always this one but  
[0:25:56] we also want to find some how to condition this  model into building something that we want to oh  
[0:26:02] and the idea is that we modify this unit so  this we this model here Epsilon Theta will be  
[0:26:12] built using uh let me show you this unit model  here this unit will receive as input an image  
[0:26:20] that is noisified so for example a cat and at a  particular with a particular noise level and we  
[0:26:28] also tell him what is the noise level that we  added to this cat and we give them both to the  
[0:26:32] input of the unit and the unit has to predict how  much noise is there this is the job of the unit  
[0:26:39] what if we introduce also the prompt signal here  so the conditioning signal here so the prompt  
[0:26:47] this way if we tell the model can you remove noise  from this image which is which has this quantity  
[0:26:55] of noise and I am also telling you that it's a  cat so the model has more information on how to  
[0:27:01] remove the noise yes the model can learn this way  how to remove noise into and building something  
[0:27:08] that is more closer to the prompt this will make  them the model conditioned it means that it will  
[0:27:15] act like a conditioned model so we need to tell  the model what is the condition that we want so  
[0:27:20] that the model can remove the noise in that  particular way into moving the output towards  
[0:27:26] that particular prompt but at the same time when  we train the model instead of only giving images  
[0:27:33] with along with the prompt we can also sometimes  with the probability let's say 50 percent not give  
[0:27:40] any prompt and let the model remove the noise  without telling him anything about the prompter  
[0:27:46] so we just give him a bunch of zero when we  give him the input this way the model will  
[0:27:52] learn to act both as a conditioned model and  also as a conditioned model so the model will  
[0:27:59] learn to pay attention to the prompt and also  to not pay attention to the prompt and what is  
[0:28:05] the advantage of this is that we can once when  we want to generate a new picture we can't do  
[0:28:12] two steps in the first one suppose you want  to generate a picture of a character we can  
[0:28:17] do like this let me delete first of all okay we  can do the first step so let's call it step one
[0:28:26] and we can start with Pure Noise because as  I told you before to generate a new image we  
[0:28:33] start from Pure Noise We indicate the model what  is the noise level so at the beginning it will  
[0:28:38] be t equal to 1000 so maximum noise level and we  tell the model that we want the cat we give this  
[0:28:46] as in as input to the unit the unit will predict  some noise that we need to remove in order to move  
[0:28:55] the image towards what we want as output so a cat  and this is our output one let's call it output  
[0:29:02] one then we do another step so let me delete this  one then we do another step let's call it step two
[0:29:13] and again we give the same input noise as before  the same time step as the noise level so it's the  
[0:29:22] same noise with the same noise level but we don't  give any prompt this way the model will build some  
[0:29:28] output let's call it out 2 which is how to remove  the noise to generate something we don't know what  
[0:29:34] what would to generate something that belongs to  our data distribution and then we combine these  
[0:29:40] two output in such a way that we can decide how  much we want the output to be closer to the prompt  
[0:29:48] or not this is called classifier free guidance  so this approach here is called the classifier  
[0:29:56] free guidance I will not tell you why it's called  classifier for guidance because otherwise I need  
**********BOOKMARK 1***************
[0:30:00] to introduce the classifier guidance and to talk  about the classifier guidance I need to introduce  
[0:30:04] the score based models to understand why it's  called like this but the idea is this that we  
[0:30:10] train a model that when we train it sometimes we  give it the prompt and sometimes we don't give it  
[0:30:16] a prompt so that the model learns to ignore the  prompt but also to pay attention to the prompt  
[0:30:21] and when we sample from this model we do two  steps first time we give him the prompt of what  
[0:30:27] we want and the second time we give the same  noise but without the prompt of what we want  
[0:30:31] and then we combine the two output conditioned  and unconditioned linearly with a weight that  
[0:30:41] indicates how much we want the output to be closer  to our condition to over prompt the higher this  
[0:30:48] value the more the output will resemble our prompt  the lower this value the less it will resemble our  
[0:30:55] prompt and this is the idea behind classifier free  guidance to to give the prompt actually we will  
[0:31:01] give we need to give some kind of embedding  to the so the model needs to understand this  
[0:31:06] prompt to understand the prompt the model needs  some kind of embedding embedding means that we  
[0:31:13] need some vectors that represent the meaning of  the prompt and disembedding are extracted using  
[0:31:20] the clip text encoder so before talking about the  text encoder let's talk about clip so clip was a  
[0:31:27] model built by open AI that allowed to connect  text with images and the text basically they  
[0:31:36] took a bunch of images so for example this picture  and its description then they took another image  
[0:31:43] along with its description so the image one is  associated with the text number one which is  
[0:31:48] the description of the image one then the image 2  has the description number two the image 3 has the  
[0:31:55] text text number three which is the description  of the image tree etc etc they built this Matrix  
[0:32:01] you can see here which is made up of the dot  products of the embedding of the first image  
[0:32:06] multiplied with all the possible captions here  so the image one with the text one image one with  
[0:32:13] the text to image one with the text three Etc then  image two with the text 1 image 2 with the text 2  
[0:32:18] Etc how they train it basically we know that the  correspondence between image and the text is on  
[0:32:25] the diagonal because the image one is associated  with the text one image 2 is associated with the  
[0:32:31] text to image 3 is associated with the text tree  so what how they train it basically they they said  
[0:32:36] they built a loss function that they that they  want this diagonal to have the maximum value and  
[0:32:43] all the other numbers here to be zero because they  are not matching they are not the corresponding  
[0:32:48] description of these images in this way the model  learned how to combine the description of an image  
[0:32:56] with the image itself and what we do in stable  diffusion is that we take this text encoder  
[0:33:02] here so only this part of this clip to encode our  prompt to get some embeddings and these embeddings  
[0:33:11] are then used as conditioning signal for our  unit to denoise the image into what we want
[0:33:21] um okay there is another thing that we need  to understand so as I said before to we have  
[0:33:27] a forward process that adds noise to the image  then we have a reverse process that removes noise  
[0:33:32] from the image and this reverse process can be  conditioned by using the classifier free guidance  
[0:33:40] and this reverse process means that we need to  do many steps of denoisification to arrive to  
[0:33:47] the image to the new image and this also means  that each of these steps involves going through  
[0:33:55] the unit with a noisified image and getting as  output the amount of noise present in this image  
[0:34:02] but if the image is very big so suppose this  image here is 512 multiplied by 512 it means  
[0:34:10] every time on the unit we will have a very big  Matrix that needs to go through this unit and  
[0:34:18] this may be very slow because it's a very big  Matrix of data that the unit has to work on  
[0:34:23] what if we could somehow compress this image into  something smaller so that each step through the  
[0:34:31] unit takes less time well the idea is that yes  we can compress this image with something that  
[0:34:38] is called the variational auto encoder let's  see how the variational auto encoder works
[0:34:45] um okay the stable diffusion  is actually known as the  
[0:34:50] is known as a latent diffusion model because  what we learn is not the data probability  
[0:34:57] distribution PX of our data but we learned  the latent representation of the data using  
[0:35:05] a variational auto encoder so basically  we compress our data so let's go back we  
[0:35:10] compress our data into something smaller and then  we learn the the notification process using this  
[0:35:17] compressed version of the data not the original  data and then we can decompress it to build  
[0:35:22] the original data let me give let me show you  actually how it works on a practical level so  
[0:35:29] imagine you have some data and you want to send  it to your friend over the Internet what do you  
[0:35:34] do you can send the original file or you can  send the zip the file so you can zip the file  
[0:35:40] maybe with WinZip for example and then you send  the file to your friend and the friend can unzip  
[0:35:46] it after receiving and rebuild the original data  this is exactly the job of the auto encoder the  
[0:35:52] auto encoder is a network that given an image  for example will after passing through the  
[0:35:57] encoder will transform into a vector which has a  dimension that is much smaller than the original  
[0:36:03] image and if we use this vexor and run it through  the decoder it will build the original image back  
[0:36:12] and we can do it for many images and each of them  will have a representation in this let's this is  
[0:36:19] called a code corresponding to each image now  the problem with auto encoder is that the code  
[0:36:25] learned by this model doesn't make any sense from  a semantic point of view so the code associated  
[0:36:32] with the cat for example may be very simple to  the similar to the code associated with pizza  
[0:36:37] for example or the code associated with a building  so there is no semantic relationship between these  
[0:36:44] codes and to overcome this limitation of the auto  encoder we introduce the variational autoencoder  
[0:36:49] in which the we we learn to kind of compress the  data but at the same time this data is distributed  
[0:36:58] according to a multivariate distribution which  most of the times is a gaussian and we learn  
[0:37:04] the mean and the sigma of this distribution this  very complex complex distribution here and given  
[0:37:12] the latent representation we can always pass it  through the decoder to rebuild the original data  
[0:37:17] and this is the idea that we use also in stable  diffusion now we can finally combine all these  
[0:37:24] things that we have seen together to see what  is the architecture of the stable diffusion  
[0:37:31] so let's start with how the text to image  works now imagine a text to image basically  
[0:37:38] works like this imagine you want to generate  a picture of a dog with glasses so you start  
[0:37:45] of course with a prompt a dog with glasses  and then what do you do we sample some noise  
[0:37:52] here some noise from the n01 we encode it with  our variational encoder this will give us a latent  
[0:38:00] representation of this noise let's call it Z this  is of course a Pure Noise but has been compressed  
[0:38:08] by the encoder and then we send it to the unit  the goal of the unit is to detect how much noise  
[0:38:16] is there and also because to the unit we also give  the conditioning signal the unit has to detect the  
[0:38:23] the noise how much what noise we need to remove  to make it into a picture that follows the prompt  
[0:38:30] so into a picture of a dog so the unit we pass it  through the unit along with the time step initial  
[0:38:36] time step so 1000 and the unit will detect at the  output here how much noise is there our scheduler  
[0:38:44] we will see later what is the scheduler will  remove this noise and then send it again to  
[0:38:49] the unit for the second step of denucification  and again we send the time step which is in this  
[0:38:55] case not of nine one thousand but 980 for example  because we skip some steps and then we again with  
[0:39:03] the Noise We detect how much noise is there the  scheduler will remove this noise and again send  
[0:39:08] it back and we do many times this we keep doing  this denucification for many steps until there  
[0:39:16] is no more noise present in the image and after we  have finished this Loop of steps we get the output  
[0:39:25] Z Prime which is still a latent because this  unit only works with the latent representation  
[0:39:30] of the data not with the original data we pass  it through the decoder to obtain the output image  
[0:39:38] and this is why this is called a latent  diffusion model because the unit so that  
[0:39:43] the noisification process always works with the  latent representation of the data and this is how  
[0:39:48] we generate text to image we can do the same  thing for image to image image to image means  
[0:39:54] that I have for example the picture of a dog and  I want to modify this image into something else  
[0:40:01] by using a prompt for example I want the model  to add glasses to the stock so I can give the  
[0:40:06] input image here and then I say a dog with glasses  and hopefully the model will add glasses to this  
[0:40:12] dog how does it work we encode the image with the  encoder of the variational of the encoder and we  
[0:40:19] get the latent representation of our image then we  add the noise to this latent because the unit as  
[0:40:26] we saw before his job is to denoise an image but  of course we need to have some noise to the noise  
[0:40:32] so we add the noise to this image and the amount  of noise that we add to this image so this  
[0:40:39] starting image here indicates how much Freedom  the unit has into building the output image  
[0:40:45] because the more noise we add the more the unit  has freedom to alter the image but the less noise  
[0:40:51] we add the less Freedom the model has to alter the  image because it cannot change radically the the  
[0:40:58] um if you we start from Pure Noise the the unit  can do anything it wants but if we start with  
[0:41:04] less noise the unit is forced to modify just a  little bit of the output image so the there are  
[0:41:11] the amount of noise that we start from indicates  how much we want the model to pay attention to the  
[0:41:16] initial image here and then we give the prompt  for many steps we keep the noising the noising  
[0:41:23] the noising the noising and after there is no more  noise we take this latent representation we pass  
[0:41:29] it through the decoder and we get the output image  here and this is how image to image works now  
[0:41:37] let's go to the last part which is how in painting  works in painting Works similar way to the image  
[0:41:44] to image but with a mask so in painting means  first of all that we have an image and we want to  
[0:41:50] cut some part of this image for example the legs  of this dog and we want to the model to generate  
[0:41:55] new legs for this dog that are maybe a little  different so as you can see this that this feet  
[0:42:01] here are a little different from the the the legs  of the dog here so what we do is we start from  
[0:42:09] our initial image of the dog we pass it through  the encoder it becomes a latent representation  
[0:42:14] we add some noise to this latent representation we  give some prompt on to tell the model what we want  
[0:42:21] the model to generate so I just say a dog running  because I want to generate new legs for this dog  
[0:42:26] and then we pass the nullified input to the  unit the unit will produce an output Here For  
[0:42:35] the First Time step but then of course nobody  told the model to only predict this area the  
[0:42:42] model of course here at the output predicted  and modified the denois the old image but we  
[0:42:49] take this output here and we remove we we don't  care what the the noise predicted for this area  
[0:42:57] of the image the area that we already know we  replace it with the image that we already know  
[0:43:04] and we pass it again through the unit basically  what we do with at every step at every output of  
[0:43:11] the unit we replace the areas that are already  known with the areas of the original image so to  
[0:43:19] basically to fool the model into believing that  it was the model itself that came up with this  
[0:43:24] details of the image not as so every time here in  this area before we send it back to the unit here  
[0:43:33] here we combine the output of the unit with the  existing image by replacing whatever output the  
[0:43:41] unit give us for this area here with what is  the original image and then we give it back to  
[0:43:47] the unit unit and we keep doing it this way the  model will be only be able to work on this area  
[0:43:53] here because this is the one we never replace  in the output of the unit and then after there  
[0:43:58] is no more noise we take the output we send  it to the decoder and then it will build the  
[0:44:05] image we can see here okay this is how the  stable diffusion work from an architecture  
[0:44:12] point of view I know it has been a long journey  I had to introduce many Concepts but it's very  
[0:44:17] important that we know these Concepts before  we start building the unit because the the  
[0:44:23] model otherwise we we we don't even know  how to start building the stable diffusion  
[0:44:29] here we are finally coding our stable diffusion  and the first thing that we will code is the  
[0:44:35] variational autoencoder because it's external to  the unit so it's external to the diffusion model  
[0:44:40] so the one that will detect will predict how much  noise is present in the image and let's review  
[0:44:46] it actually let's review the architecture  and let me go to this slide here okay oops
[0:44:58] this one here okay the first thing  that we will build is this part here  
[0:45:04] the encoder and the decoder of our variational  of the encoder the job of the encoder and the  
[0:45:11] decoder of the version of decoder is to encode  an image or noise into a compressed version of  
[0:45:18] the image or the noise itself such that then we  can take this latent and run it through the unit  
[0:45:25] and then after the last step of the notification  we take this compressed version or latent and we  
[0:45:33] pass it through the decoder and to get the  original the output image not the original  
[0:45:38] and um so the encoder actually his job is to  reduce the dimension of the data into a smaller  
[0:45:46] data into the data with the smaller Dimension  and the idea is very similar to the one of the  
[0:45:52] units so we start with the picture that is very  big and at each step there are multiple levels  
[0:45:57] we keep reducing the size of the image but at the  same time we keep increasing the features of the  
[0:46:05] image what does it mean that initially each pixel  of the image will be represented by three channels  
[0:46:11] so red green and blue RGB at each step by using  convolutions we will reduce the size of the image  
[0:46:19] but at the same time we will increase the number  of features that are that each pixel represents  
[0:46:26] so each pixel will be represented not by three  channels but maybe but but by more channels this  
[0:46:31] means that each pixel will actually capture more  data more data of the area to which that pixel  
[0:46:39] belongs and this is thanks to the convolutions but  I will see I will show you later with an animation  
[0:46:46] and so let's start building the first  thing we do is open Visual Studio okay  
[0:46:52] and we create three folders the first is  called data and later we downloaded the  
[0:46:57] pre-trained weights that you can also find  on my GitHub another folder called images  
[0:47:03] in which we put images as input and output  and then another fold called SD which is our  
[0:47:09] model let's create two files one called encoder  dot pi and one called decoder dot Pi these are  
[0:47:17] these are the encoder and the decoder over of  our Virtual Router encoder let's start with  
[0:47:24] the encoder and then the encoder is quite  simple so let's start by importing torch
[0:47:33] and all the other stuff
[0:47:38] oops
[0:47:45] let me also select The Interpreter
[0:47:50] okay then we need to import two other blocks  that we will Define later in the decoder let's  
[0:47:57] call them for now Port VA attention block and VA  residual block for those who are familiar with  
[0:48:09] the computer vision models the resistor block  is very similar to the residual block that is  
[0:48:12] used in the resnet so later you will see the  structure it's very similar but if those who  
[0:48:17] are not familiar don't worry I will explain  it later so let's start building this encoder
[0:48:24] and this will inherit from the sequential module  
[0:48:27] which means basically our encoder  is a sequence of models supermodels
[0:48:48] okay it's a sequence of submodels in which each  module is something that reduces the dimension  
[0:48:55] of the data but at the same time increases its  number of features I will write the um I will  
[0:49:05] write the blocks one by one and as soon as we  encounter a block that we didn't Define we go  
[0:49:11] to Define it and then we Define also the shapes so  the first thing we do just like in the unit is a  
[0:49:17] redefine a convolution convolution 2D and  initially our image will be have will have  
[0:49:24] three channels and we convert it to 128 Channels  with the kernel size of 3 and the padding of one  
[0:49:33] for those who are not familiar with convolutions  let's go have a look at how convolutions work
[0:49:54] here
[0:49:58] here we can see that a convolution basically it's  a kernel so it's made of a matrix of a size that  
[0:50:06] we can decide which is defined by the parameter  kernel size which is run through the image as in  
[0:50:14] the following animation so block by block as you  can see and at each block each of the pixel below  
[0:50:22] the kernel is multiplied by the value of the  kernel in that position so in this for example  
[0:50:28] this pixel here which is in position let's call  the let's say this this one here so the first row  
[0:50:34] and the First Column is multiplied by the this red  value of the kernel the second column first row is  
[0:50:42] multiplied by the green value of the kernel and  then all of these multiplications are summed up  
[0:50:47] to produce one output so this output here comes  from four multiplications that we do in this  
[0:50:53] area each one with the corresponding number of the  kernel this way basically by running this kernel  
[0:51:00] to the image we capture local informations about  the image and this pixel here combines somehow  
[0:51:07] the information of 4 pixels not only one and  that's it then we can also increase the kernel  
[0:51:15] size for example and the kernel size will be  increasing the kernel means that we capture  
[0:51:20] more Global Information so each pixel represents  the information of more pixel from the original  
[0:51:26] picture so the output is smaller and then we can  introduce for example The Stride which means that  
[0:51:33] we don't do it every successive pickles pixel  but we skip some pixels as you can see here so  
[0:51:39] we skip every second pixel here and if the number  is uh the the kernel size is even and the input  
[0:51:47] is the size is other we will also never touch for  example here the Border as you can see we can also  
[0:51:54] implement the deletion which means that it becomes  the with the same kernel size the information  
[0:52:00] becomes even more Global because we don't watch  a consecutive pixel but we skip some pixels etc  
[0:52:07] etc so the kernels basically in the convolutions  allow us to capture information from a local area  
[0:52:13] of the picture of the image and combine it using  a kernel and this is the idea behind convolutions  
[0:52:21] so this convolution here for example we'll  start with our okay let's define some shapes  
[0:52:27] our variational autoencoder so the encoder  of the variational auto encoder will start  
[0:52:32] with batch size and the three into three channels  let's define it as Channel then we this the image  
[0:52:42] will have a height and the width which will  be 512 by 512 as we will see later and this  
[0:52:48] convolution will convert it into batch size 128  features with the same height and the same width  
[0:52:58] um why in this case the height and the  width doesn't change because even if we  
[0:53:03] have a kernel size of size 3 because we had  padding basically we add something to the  
[0:53:09] right side something to the top side something  to the bottom and the left of the image so the  
[0:53:13] image with the padding becomes bigger but then  the output of the convolution makes it smaller  
[0:53:18] and matches the original size of the image this  is the the reason we have the padding here but  
[0:53:25] we will see later that with the next blocks  the image size will start becoming smaller  
[0:53:32] the next block is called the residual  block and the Bae residual block  
[0:53:39] which is from 188 to 28 channels to 128 channels  this is a combination this residual block is a  
[0:53:47] combination of convolutions and normalization so  it's just a bunch of convolutions that we will  
[0:53:55] Define later and this one indicates how many input  channels we have and how many output channels we  
[0:54:02] have and the resistor block will not change the  size of the image so we Define it so our input  
[0:54:09] image is 128 so right size 128 hate and width  and it becomes it Remains the Same basically oops
[0:54:25] okay we have another one another residual  block with so the same transformation  
[0:54:34] then we have another convolution and  this time the convolution will change  
[0:54:37] the size of the image and we will see  why so we have a convolution to the
[0:54:43] 128 to 128 because the output channels  of the last and the last block is 128  
[0:54:51] so the input channel is 128 the output is 128.  
[0:54:55] the kernel size is three The Stride is two and  the padding is zero this will basically introduce  
[0:55:05] kernel size 3 stride 2 that's watch so imagine  the batch size is six by six kernel size is  
[0:55:12] 3 stride is 2 without the deletion and  this is the output let me make it bigger
[0:55:23] okay something yeah so as you can see with the  stride of two uh need to make it okay with the  
[0:55:36] stride of two and the kernel size of three  this is the behavior so we skip every two  
[0:55:40] pixels before calculating the output and this  makes the output smaller than the input because  
[0:55:47] of this stride and also because of the kernel  size and we don't have any padding so this ah  
[0:55:54] transformation here will have the following  shapes so we are starting from batch size 128
[0:56:04] width so the original height  and the width of the input image  
[0:56:08] but this time it will become batch size 128
[0:56:17] the head will become half and  the width will become half
[0:56:25] Etc then we have two more  residual blocks with the same
[0:56:33] as before but by this time by  increasing the number of features
[0:56:41] and also here we don't increase any here  by increasing the feature means that we  
[0:56:47] don't increase the size of the image or  we reduce the size of the image we just  
[0:56:51] increase the number of features so this  one becomes 256. and here we start from
[0:57:04] oops 256 and we remain to 56. now you may  be confused of why we are doing all of this  
[0:57:10] okay the idea is we start with the initial  image and we keep decreasing the size of  
[0:57:15] the image so later you will see that the  image will become divided by 4 divided by  
[0:57:18] 8 but at the same time we keep increasing  the features so each pixel represents more  
[0:57:25] information but the number of pixels is  diminishing is is reducing at every step
[0:57:33] so let's go forward then  we have another convolution  
[0:57:38] and this time the the size will become divided  by 4 and the convolution is let me copy this one
[0:57:48] 256 by 256 because the previous output is 256  the kernel size is 3. let's try these two and  
[0:57:57] the padding is zero so just like before also  in this case the size of the image will become  
[0:58:02] half of what is it now so the image is already  divided by two so it will become divided by 4 now
[0:58:14] then we have another um digital block in  which we increase the number of features  
[0:58:22] this time from 256 to 512 so we start  from 256 and the image is divided by four  
[0:58:32] and we go to 512 and the image size doesn't change
[0:58:40] then we have another one from  512 to 512 in this case oops  
[0:58:50] we will see later what is the residual block but  the result block you have to think of it as just  
[0:58:55] a convolution with the normalization we will see  later and this one is 512 and that goes into 512.  
[0:59:04] and then we have another convolution that  will make it uh even smaller so let's copy  
[0:59:11] this convolution here this one will go from 512  to 512 the same kernel size and the same stride  
[0:59:21] and the same padding as before so the image  will become even smaller so our last Dimension  
[0:59:28] was this let me copy it so we start with an  image that is 512 four times smaller than the  
[0:59:35] original image and with the four times smaller  width it will it will become eight times smaller  
[0:59:43] and that's it and then we have a residual blocks  also here we have three of them in this case
[0:59:53] let me copy
[0:59:56] one two two three I just write the one  for the last one so anyway the size is  
[1:00:04] the shape changes here it doesn't change the  shape of the image or the number of features  
[1:00:10] so here we are going from divide by 8 and  512 here 512 and we go to same Dimension 512
[1:00:27] divided by 8 and divide by 8. then we have  annotation block and later we will see what  
[1:00:33] is the attention block basically it will run  a self-attention over each pixel so each pixel  
[1:00:39] will become kind of as you remember that the  attention is a way to relate tokens to each  
[1:00:44] other in a sentence so if we have an image made  of pixels the attention can be thought of as  
[1:00:50] a sequence of peak cells and the attention  as a way to relate the pixel to each other  
[1:00:55] so this is the goal of the attention block
[1:01:00] and because this way the the each pixel is  related to each other is not independent  
[1:01:05] from each other even if the convolution already  actually related relates close pixels to each  
[1:01:12] other but the attention will be a global so even  the last pixel can be related to the first pixel  
[1:01:17] this is the goal of the attention block and also  in this case we don't reduce the size because  
[1:01:23] the attention is the Transformer's attention is a  sequence to sequence model so we don't reduce the  
[1:01:30] size of the sequence and the the image Remains  the Same finally we have another visitor block
[1:01:40] s
[1:01:43] let me copy here also no change in shape or size  of the image then we have a normalization and we  
[1:01:50] will see what is this normalization it's the group  normalization which also doesn't change the size  
[1:01:56] just like any normalization by the way
[1:02:01] with the number of groups being 32 and the  number of channels being 512 because it's  
[1:02:07] the number of features finally we have  an activation function called the silu  
[1:02:12] The Silo is a function okay it's derived from  the sigmoid linear unit and it's a function just  
[1:02:19] like the relu there is nothing special they just  saw that this one works better for this kind of  
[1:02:24] application but there is now particular reason to  choose one over another except that they so that  
[1:02:33] practically this one works fine for this kind of  models and if you watch my previous video about  
[1:02:40] the Llama for example in which we analyze why we  they chose the Ziggler function if you read the  
[1:02:44] paper at the end of the paper they say that there  is no particular reason they chose this Igloo they  
[1:02:49] just saw that practically it works better I  mean it's very difficult to describe why a  
[1:02:53] activation function works better than the others  so this is why they use the Zillow here because  
[1:02:58] practically it works well now we have another two  convolutions and then we are done with the encoder
[1:03:08] convolution
[1:03:11] 512 8 current size and then heading
[1:03:19] this will uh not change the size of the  model because just like before we have  
[1:03:24] the kernel size S3 but we have the padding  that compensates for the reduction given  
[1:03:29] by the kernel size but we are decreasing the  number of features and this is the bottleneck  
[1:03:34] of the encoder and I will show you later  on the architecture what is the bottleneck
[1:03:45] and finally we have another convolution
[1:03:49] which is eight by eight with kernel size  equal to one and the padding is equal to zero  
[1:03:58] which also doesn't change the the the  size of the image because if you watch  
[1:04:03] here if you have a kernel size of  one means that each without Strider  
[1:04:09] each kernel basically is running over each  pixel so each output actually captures  
[1:04:15] the information of only one pixel so the  output has the same Dimension as the input  
[1:04:20] and this is why here also we don't change  the but here we need to change the number of
[1:04:31] it becomes eight and here from 8 to 8. and this is  the list of models that will make up our encoder  
[1:04:45] before building the residual block and the  attention block so this attention block let's  
[1:04:51] write the forward method and then we build  the the reservo block so this is the init  
[1:04:59] I Define it like this
[1:05:02] let me review it if it's correct okay yeah
[1:05:11] now let's define the forward method
[1:05:16] X is the image for which we wanted  that we want to encode so it's a tensor  
[1:05:21] torch dot answer and the noise we  need some noise and later I will  
[1:05:26] show you why we need some noise that has  the same size as the output of the encoder
[1:05:33] this returns a tensor
[1:05:37] okay our input X will be of size batch size  
[1:05:44] with some channels and initially it  will be three because it's an image  
[1:05:48] height and width which will be 512 by 512 and  then some noise this noise has the same size  
[1:05:56] as the output of the encoder and we will see  that it's jelly batch size then output channels  
[1:06:10] height divide by 8 and width divide by 8. then  we just run sequentially all of these modules
[1:06:26] and then there is one little thing here that  um in the convolutions that have destroyed  
[1:06:36] we need to apply a special embed padding  and I will show you why and how it works
[1:06:46] so if the model has a stride attribute and  it's equal to 2 2 which basically means this  
[1:06:55] convolution here this convolution here and this  convolution here we don't apply the padding here  
[1:07:01] because the padding here is applied to the top of  the image bottom left and right but we want to do  
[1:07:08] in a symmetrical padding so we do it manually  and this is applied like this F dot padding
[1:07:19] basically this says can you add a layer of  pixels on the right side of the image and  
[1:07:28] on the bottom side of the image only  because when you apply the padding  
[1:07:33] it's padding left padding right padding top
[1:07:40] padding bottom this means add a layer of  pixels in the right side of the image and  
[1:07:47] on the top side of the image and and this is  It's a asymmetrical padding and then if it's  
[1:07:55] we apply it only for this convolutions  that have this stride equal to two  
[1:08:01] and then we X is equal to module of X and okay now  you may be wondering why are we building this kind  
[1:08:09] of structure why it's made like this okay usually  in deep learning communities um especially during  
[1:08:16] research we don't reinvent the wheel every time so  the people who made the stable diffusion but also  
[1:08:22] the people before them every time we want to use  a model we check what models similar to the one we  
[1:08:28] want to build are already out there and they are  working fine so very probably the people who build  
[1:08:34] stable diffusion they saw that a model like this  is working very well for some previous project as  
[1:08:41] a variational autoencoder they just modified it a  little bit and kept it like it so for most choices  
[1:08:48] actually there is no reason there is a historical  reason because it works well in practice and we  
[1:08:55] know that convolutions work well in practice for  image segmentation for example or anything related  
[1:09:00] to computer vision and this is why they they made  the model like this so most encoders actually work  
[1:09:07] like this that we reduce the size of the image  but each we keep increasing the features of the  
[1:09:12] image the channels the number of channels of the  image so the number of pixels becomes smaller but  
[1:09:19] each pixels is represented by more than more than  three channels so more channels at every step now  
[1:09:25] what we do is here we are running our image into  sequentially in one by one um through all of these  
[1:09:36] um models here so first through this convolution  then through this residual block which is also  
[1:09:41] some convolutions then this residual block then  again convolution convolution convolution until we  
[1:09:47] run it through this attention block and Etc this  will transform the image into something smaller so  
[1:09:55] a compressed version of the image but as I saw  you before this is not an auto encoder this is  
[1:10:01] a variational auto encoder so the variational  auto encoder let me show you again the picture
[1:10:08] here we are not learning how to compress data  we are learning a latent space and this latent  
[1:10:14] space is a are the parameters of a multivariate  gaussian distribution so actually the variational  
[1:10:21] autoencoder is trained to learn the MU and  the sigma so the mean as the variance of this  
[1:10:27] distribution and this is actually what we will  get from the output of this variational out encode  
[1:10:35] or not directly the compressed image and if this  is not clear guys I made a previous video of the  
[1:10:43] about the variational encoder in which I show  you also why the history of why we do it like  
[1:10:48] this although the parameters the parametrization  trick Etc but for now just remember that this is  
[1:10:54] not just a compressed version of the image  it's actually a distribution and then we  
[1:10:59] can sample from this distribution and I will  show you how so the output of the variational  
[1:11:05] encoder is actually the mean and the variance and  actually it's actually not the variance but the  
[1:11:10] log variance so the mean as the log variance is  equal to torch dot chunk x 2 Dimension equal 1.  
[1:11:23] we will see also what is the chunk function so  I will show you so this basically converts batch  
[1:11:29] size eight channels height all right divide by  eight with divided by 8 which is the output of  
[1:11:38] the last layer of this encoder so this one  and we divide it into two sensors so this  
[1:11:45] chunk basically means divided into two tensors  along this Dimension so along this Dimension  
[1:11:51] it will become two tensors of size along this  dimension of size 4. so two then source of shape  
[1:12:00] batch size four then height divide by  eight and with oops with divided by eight
[1:12:15] and this basically the output of this  actually represents the mean and the variance  
[1:12:21] and what we do we don't want the um the log  variance we wanted the variance actually so to  
[1:12:28] transform the log bar the the log variance  into variance we do the exponentiation  
[1:12:33] so the first thing actually we also need to do  is to clamp this variance because otherwise it  
[1:12:38] will become very small so clamping means that  if the variance is too small or too big we want  
[1:12:43] it to become within some ranges that we are  acceptable for us so this clamping function  
[1:12:49] log variance tells the pi torch that if the value  is too small or too big make it within this range  
[1:12:57] and this doesn't change the shape of the tensors  so this Still Remains this tensor here and  
[1:13:06] then we transform the log variance into variances  so the variance is equal to the log variance dot  
[1:13:13] X which means to make the exponential of this so  you delete the log and it becomes the variance  
[1:13:20] and this also doesn't change the size of  the the shape of the tensor and then to  
[1:13:26] calculate the standard deviation  from the variance as you know the  
[1:13:28] standard deviation is the square root  of the variance so standard deviation
[1:13:37] is the variance Dot sqrt and also this doesn't  change the size of the tensor okay now what we  
[1:13:49] want as I showed you before this is the latent  space it's a multivariate gaussian which has its  
[1:13:54] own mean and its own variance and we know the mean  and the variance this mean and this variance how  
[1:14:00] do we convert how do we sample from it well what  we can sample from is basically we can sample from  
[1:14:08] n01 this is we if we have a sample from n01 how  do we convert it into a sample of a given mu so  
[1:14:16] a given mean and the given variance this as if  you remember from probability and statistics if  
[1:14:23] you have a sample from n01 you can convert it into  any other sample of a gaussian with a given mean  
[1:14:29] and the variance through this transformation so if  Z let's call it this one Z is equal to n01 we can  
[1:14:37] transform into another n let's call it X through  this transformation X is equal to Z uh well the  
[1:14:47] mean of the new distribution plus the standard  deviation of the new distribution multiplied by  
[1:14:54] Z this is the transformation this is the formula  from probability statistics basically means  
[1:14:59] transform this distribution into this one this  has this mean and this variance which basically  
[1:15:03] means sample from this distribution this is why  we are given also the noise as input because the  
[1:15:09] noise we want it to come from with a particular  seed of the noise generator so we ask is as a  
[1:15:16] input and we sample from this distribution like  this x is equal to mean plus standard deviation  
[1:15:23] multiplied by noise finally there is also another  step that we need to scale the output by a  
[1:15:33] constant this constant I found it in the original  repository so I'm just writing it here without any  
[1:15:40] explanation on why because I actually I also don't  know it's just a scaling constant that they use at  
[1:15:46] the end I don't know if it's there for historical  reason because they use some previous model that  
[1:15:50] had this constant or they introduced it for some  particular reason but it's a constant that I saw  
[1:15:55] it in the original repository and actually if  you check the original parameters of the stable  
[1:16:00] diffusion model there is also this constant so I'm  also scaling the output by this constant and then  
[1:16:07] we return X so now what we built so far except  that we didn't build the residual block and the  
[1:16:13] attention block here we built the encoder part of  the variational auto encoder and also the sampling  
[1:16:19] part so we take the image we run it through the  encoder it becomes very small it will tell us  
[1:16:25] the mean and the variance and then we sample from  that distribution given the mean and the variance  
[1:16:31] now we need to build the decoder along with the  resistor block and the attention block and what we  
[1:16:37] will see is that in the decoder we do the opposite  of what we did in the in the encoder so we will  
[1:16:43] reduce the number of channels at the end at the  same time we will increase the size of the image
[1:16:51] so let's go to the decoder let  me review if everything is fine
[1:16:59] looks like it did so let's go to the decoder
[1:17:06] again import torch
[1:17:24] we also need to define the attention
[1:17:29] we need to define the self  attention later we Define it
[1:17:36] Define first the residual  block the one we defined before  
[1:17:40] so that you understand what is this residual block
[1:17:45] and then we Define the attention block that we  defined before and finally we build the attention  
[1:17:50] so
[1:18:05] okay this is made up of normalization  convolutions like I said before  
[1:18:10] there is a two normalization  which is the group Norm one
[1:18:39] and then there is another group normalization
[1:18:55] these remote channels to Old channels
[1:19:04] and then we have a skip connection  skip connection basically means that  
[1:19:08] you take the input you skip some layers and  then you connect it there with the output of  
[1:19:13] the last layer and we also need this residual  connection if the two channels are different  
[1:19:19] we need to create another intermediate  layer now I created later I explain it
[1:19:49] okay let's create the forward method
[1:19:57] which is the torch.tensor
[1:20:02] and Returns the torch dot tensor okay the  input of this residual layer as you saw before  
[1:20:11] is something that has a batch with some  channels and then height and width which  
[1:20:17] can be different it's not always the same  sometimes it's 512 by 512 sometimes it's  
[1:20:22] half of that sometimes one-fourth of that  Etc so suppose it's a X is a batch size
[1:20:32] in channels height with what we do is we create  the script connection so we save the initial  
[1:20:41] input we call it the residual or residue  is equal to X we apply the normalization
[1:20:52] the first one and this doesn't  change the shape of the the tensor  
[1:20:57] the normalization doesn't change  then we apply the zero function
[1:21:04] and this also doesn't change the size of the  tensor then we apply the first convolution
[1:21:15] this also doesn't change the size of the tensor  because as you can see here we have kernel size  
[1:21:20] 3 yes but with a padding of one with the padding  of one actually it will not change the size of the  
[1:21:26] tensor so it will still remain this one then  we apply again the group normalization Tool
[1:21:35] this again doesn't change the size of  the tensor then we apply The Silo again
[1:21:43] then we apply the convolution number two
[1:21:51] and finally we apply the residual connection  which basically means that we take X plus the  
[1:21:58] residual residual but if the number of output  channels is not equal to the input channels  
[1:22:05] you cannot add this one with this one because  this Dimension will not match between the two  
[1:22:10] so what we do we create this layer here to convert  the input channels to the output channels of X  
[1:22:17] such that this sum can be done so what we do is  we apply this residual layer residual layer of  
[1:22:25] residue like this and this is our residual  block so as I told you it's just a bunch  
[1:22:31] of convolutions and group normalization  and for those who are familiar with the  
[1:22:35] computer vision models especially in resnet  we use a lot of it it's a very common block  
[1:22:43] let's go build the rotation block that we used  also before in the encoder this one here and to  
[1:22:50] define the attention we also need to define the  self-attention so let's first build the attention  
[1:22:55] block which is using the variational autoencoder  and then we Define what is this self-attention
[1:23:26] it has a group normalization again  the channel is always 32 here in  
[1:23:32] in stable diffusion but you also may be wondering  what is group normalization right so let's go to  
[1:23:38] review it actually since we are here and okay  if you remember from my previous slides on  
[1:23:47] llama let's go here when we use the where we use  a layer normalization and um also in the vanilla  
[1:23:56] Transformer actually we used normalization so  first of all what is normalization normalization  
[1:24:01] is basically when we have a deep neural network  each layer of the network produces some output  
[1:24:06] that is fed to the next layer now what happens  is that if the output of a layer is a varying  
[1:24:13] in distribution so sometimes for example the  output of a layer is between 0 and 1 but the  
[1:24:19] next step maybe is between 3 and 5 and that the  next step maybe is between the 10 and 15 Etc so  
[1:24:26] the distribution of the output of a layer changes  then the next layer also will see some input that  
[1:24:32] it it's very different from what the layer is used  to see this will basically push the output of the  
[1:24:39] next layer into a new distribution itself which  in turn will push the loss function into basically  
[1:24:47] the output of the model to oscillate between these  three to change very frequently in distribution so  
[1:24:54] sometimes this will be very big number sometime it  will be a very small number sometimes it will be  
[1:24:58] negative sometime it will be positive Etc and this  basically makes the loss function oscillates too  
[1:25:03] much and it makes the training slower so what we  do is we normalize the values before feeding them  
[1:25:09] into layers such that each layer always see the  same distribution of the data so it will always  
[1:25:14] see numbers that are distributed around 0 with a  variance of one and this is the job of the layer  
[1:25:20] normalization so imagine you are a layer and you  have some input which is a batch of 10 items each  
[1:25:27] item have some features so feature one feature  two feature three layer normalization calculates  
[1:25:33] a mean and the variance over these features here  so over this distribution here and then normalizes  
[1:25:40] this value according to this formula so each each  value basically becomes distributed between 0  
[1:25:46] and 1. with batch normalization we normalize by  columns so the statistics mean and the sigma is  
[1:25:53] calculated by columns with layer normalization it  is calculated by rows so each item independently  
[1:25:59] from the others with group normalization on the  other hand it is like layer normalization but not  
[1:26:07] all of the features of the item but grouped so  for example imagine you have four features here  
[1:26:13] so here you have a F1 F2 F3 F4 and you have  two groups then the first group will be F1  
[1:26:20] and F2 and the second group will be F3 and F4 so  you will have two means and two variants one for  
[1:26:27] the first group one for the second group but why  we use it like this why do we want to group this  
[1:26:35] kind of features because these features actually  they come from convolutions and as we saw before  
[1:26:40] let's go back to the website imagine you have  a kernel of five here each output here actually  
[1:26:48] comes from local area of the image so the two  close features for example two things that are  
[1:26:56] close to each other may be related to each other  so two things that are far from each other are not  
[1:27:01] related to each other this is why we can group  we can use a group normalization in this case  
[1:27:08] because closer features to each other will have  a kind of the same distribution or we make them  
[1:27:13] have the same distribution and things that  are far from each other may not this is the  
[1:27:18] basic idea behind group normalization but  the whole idea behind the normalization is  
[1:27:22] that we don't want these things to oscillate  too much otherwise the loss of function will  
[1:27:26] oscillate and will make the training slower  with normalization we make the training faster  
[1:27:31] so let's go back to coding so we were coding the  attention block so now the attention block has  
[1:27:38] this group normalization and also an attention  which is a self-attention and later we Define it  
[1:27:45] and channels okay this one have a forward method
[1:27:54] tensor the terms of course the torch.penser okay  what is the input of this block the input of this  
[1:28:03] block is something where is where is it here it's  something in the form of a batch size number of  
[1:28:10] channels height and width but because it will  be used in many positions this attention block  
[1:28:15] we don't Define a specific size so we just  say that X is something that is a batch size  
[1:28:21] features or channels if you want height and  width again we create a residual connection
[1:28:32] and the first thing we do is we extract  the shape so n is the batch star is the  
[1:28:37] number of channels the height and the width  is equal to x dot shape then as I told you  
[1:28:45] before we do the self attention between all the  pixels of this image and I will show you how
[1:28:56] this will transform distance over here
[1:29:01] into this tensor here height multiplied by  width so now we have a sequence where each  
[1:29:10] item represents a pixel because we multiplied  height by width and then we transpose it so  
[1:29:15] put it back a little before transpose the  -1 with -2 this will transform this shape
[1:29:25] into this shape so we put back this one so this  one comes before and features becomes the last one  
[1:29:36] until like this and okay so as you can see  from this sensor here this is like when we  
[1:29:45] do the attention in the Transformer model so  in the Transformer model we have a sequence of  
[1:29:49] tokens so each tokens representing for example  a word and the attention basically calculates  
[1:29:55] the attention between each token so how do two  tokens are related to each other in this case  
[1:30:00] we can think of it as a sequence of pixels  each pixel with its own embedding which is  
[1:30:06] the features of that pixels and we relate pictures  pixels to each other and then we do the attention
[1:30:15] which is the self-attention in which  cell potential means that the query  
[1:30:19] key and values are the same input  and this doesn't change the shape  
[1:30:27] so this one Remains the  Same then we transpose back
[1:30:34] and we do the inverse transformation  so because we we put it in this form  
[1:30:38] only to do the attention so now  we transpose so we take this one
[1:30:46] and we convert it into features and  then height and width and then again  
[1:30:53] we remove this multiplication by  viewing again the tensor so nchw
[1:31:03] so we go from here
[1:31:09] to here then we add the residual connection  
[1:31:15] and we return X that's it the visual connection  will not change the size of the of the input and  
[1:31:22] we return a tensor of this shape here let me check  also the result of reaction here is correct okay  
[1:31:28] now with that have also been the attention block  let's build also the self attention since we are  
[1:31:33] building the attentions and the attentions  because we have two kind of attention in the  
[1:31:38] um stable diffusion 1 is called the  self-attention and one is the cross  
[1:31:43] attention and we need to build both so let's go  build it in a separate class called attention
[1:31:52] and okay so again import torch
[1:32:23] okay I think you guys maybe want to review the  attention before building it so let's go review  
[1:32:28] it I have here open my slides from my video about  the attention model for the Transformer model  
[1:32:35] so the self attention basically it's a way for um  to talk to in a especially in a language model is  
[1:32:42] a way for uh for us to relate tokens to each  other so we start with a sequence of tokens  
[1:32:47] each one of them having an embedding of size  D model and we transform it to into queries  
[1:32:52] key and values in which query can values in  the self-attention are the same Matrix same  
[1:32:57] sequence we multiply them by WQ Matrix so wqw  K and WV which are parameter matrices then we  
[1:33:06] split them along the D model Dimension into number  of heads so we can specify how many heads we want  
[1:33:14] in our case the one attention that we will do here  is actually only one head I will show you later  
[1:33:20] and then we calculate the attention for each of  this head then we combine back by concatenating  
[1:33:27] this head together we multiply this output  Matrix of the concatenation with another  
[1:33:33] Matrix called wo which is the output Matrix  and then this is the output of the multi-head  
[1:33:39] attention if we have only had one head instead  of being a multi-head then we will not do this  
[1:33:46] splitting operation we will just do this  multiplication with the w and with the w o  
[1:33:52] and okay this is how the self attention works  so in the self-attention we have this query  
[1:33:57] key and values coming from the same Matrix  input and this is what we are going to build
[1:34:10] so we have the number of heads then we have  the embedding so how what is the embedding of  
[1:34:16] each token but in our case we are not talking  about tokens we will talk about pixels and  
[1:34:21] we can think that the number of channels of  each pixel is the embedding of the pixel so  
[1:34:27] the the embedding just like in the original  Transformer the embeddings are the kind of  
[1:34:32] vectors that capture the meaning of the word  in this case we have the channels each Channel  
[1:34:37] um each pixel represented by many channels  that capture the information about that pixel
[1:34:45] here we have also the bias for the W matrices  which we don't have in the original Transformer
[1:35:03] okay now uh let's define the W matrices  so wqwk and WV we will represent it as  
[1:35:11] one big linear layer instead of representing  it as three different matrices it's possible  
[1:35:17] we just say that it's a big  Matrix 3 by the embedding  
[1:35:22] and the bias is if we want it so in projection in  projection bias so this stands for in projection  
[1:35:30] because we it's the projection of the input  before we apply the attention and then there  
[1:35:34] is auto projection which is after we  apply the attention so the wo Matrix
[1:35:46] so as you remember here the wo  Matrix is actually D model by  
[1:35:50] the model the input is also the  bottle by the model and this is  
[1:35:54] exactly what we did but we have three  of them here so it's three by T model
[1:36:05] and then we save the number of heads
[1:36:10] and then we saved the dimension of each head  the dimension of each head basically means  
[1:36:17] that if we have multi-head each head will  watch a part of the embedding of each token  
[1:36:22] so we need to save how much is this size so  the D model divided by the number of heads  
[1:36:30] but we divide by the number of  heads let's implement the forward
[1:36:40] we can also apply a mask as you remember the mask  is a way to avoid relating tokens that token one  
[1:36:50] particular token with the tokens that come after  it but only with the token that come before it  
[1:36:55] and this is the called the causal mask
[1:37:01] if you really are not understanding what  is happening here in the attention I  
[1:37:05] highly recommend you watch my previous video  because it's explained very well and I will  
[1:37:11] if you watch it will take not so much  time and I think you will learn a lot
[1:37:19] so the first thing we do is extract the shape
[1:37:27] then we extract but size the sequence
[1:37:33] length and the embedding is equal to input shape
[1:37:41] then we say that we will convert it into  another shape that I will show you later why  
[1:37:50] this is called the interim  shape intermediate shape
[1:38:04] then we apply the query key and value we split we  
[1:38:07] apply the in projection so the wqw  K and W V Matrix to the input and we  
[1:38:13] convert it into query key and values  so query key and values are equal to
[1:38:19] we multiply it and then we divide it so with  chunk as I showed you before what is chunk  
[1:38:25] basically we will multiply the input with  the big Matrix that represent WQ WQ and W  
[1:38:31] key but then with the we split it back into  three smaller matrices and this is the same  
[1:38:35] as applying a three different projections  instead of is the same as applying a three  
[1:38:41] separator in projections but it's also  possible to combine it in one big Matrix  
[1:38:48] this for what we will do basically  it will convert batch size
[1:38:56] sequence length Dimension into
[1:39:01] that size it's length Dimension  multiplied by three and then by  
[1:39:06] using Chunk we split it along the last  Dimension into three different than sources  
[1:39:14] of shape butt size sequence length and dimension
[1:39:23] okay now we can um  
[1:39:27] split the query key and values in the number  of heads according to the number of Heads This  
[1:39:32] Is Why We Built This shape which means split  the dimension the last Dimension into n heads
[1:39:55] and values V Dot View
[1:40:01] wonderful this will convert okay let's write  it that size sequence length Dimension into  
[1:40:14] that size sequence length then H so the  number of heads and each the dimension  
[1:40:21] divided by the number of heads so each head  will watch the full sequence but only a part  
[1:40:27] of the embedding of each token in this case  pixel and we'll watch this uh this part of  
[1:40:34] the head so the the full the dimension the  embedding divided by the number of heads
[1:40:43] and then this will convert it because we are also  transposing this will convert it into batch size  
[1:40:50] add sequence length and then  Dimension H so each head will  
[1:40:57] watch the all the sequence but  only a part of the embedding
[1:41:03] we then calculate the the attention just  like the formula so query multiplied by the  
[1:41:08] transpose of the keys so is the query matrix  multiplication with the transpose of the keys  
[1:41:17] this will return a matrix of size  
[1:41:21] batch size and sequence length by  sequence length we can then apply the mask
[1:41:30] as you remember the mask is something that we  apply when we calculate the attention if we  
[1:41:36] don't want two tokens to relate to each other  we basically substitute their value the the in  
[1:41:43] this Matrix we substitute the interaction with  minus infinity before applying the soft Max so  
[1:41:47] that the soft Max will make it zero so this is  what we are doing here we first build the mask
[1:42:04] this will create a causal mask basically  a mask where the upper triangle  
[1:42:11] so above the principal diagonal is made up of one
[1:42:20] once a lot of ones and then we  fill it up with minus infinity
[1:42:30] musket oops not a must but wait
[1:42:35] muskete fill but with mask  and we put minus infinity  
[1:42:44] like this as you remember the formula of  the Transformer is a query multiplied by  
[1:42:50] the transpose of the keys and then divided by  the square root of the model so this is what we  
[1:42:55] will do now so divided by the square root of the  model set of d add and then we apply the softmax
[1:43:15] we multiplied by the W or Matrix
[1:43:23] we transpose back so we want to remove now we want  to remove the head Dimension so output is equal to  
[1:43:34] let me write some shapes so what is this this  is equal to uh batch size sequence by sequence
[1:43:47] multiplied so matrix  multiplication with batch size
[1:43:56] this will result into
[1:44:02] that size h  
[1:44:06] silver slang and dimension divided by h this we  then multiplied by them we just then transpose
[1:44:18] and this will result into  so we start with this one
[1:44:29] and it becomes
[1:44:33] wait I put too many parentheses  here a batch size sequence length
[1:44:41] H and dimensions okay then we can reshape
[1:44:52] as the input like the initial shape so this one  
[1:44:59] and then we apply the output projection  so we multiply it by the w o Matrix
[1:45:21] okay
[1:45:24] this is the self-attention now let's go  back to continue building the decoder for  
[1:45:29] now we have built the attention block and the  residual block but we need to build the decoder
[1:45:56] and also this one is a sequence of models that  we will apply one after another we start with  
[1:46:01] the convolution just like before now I will  not write again the shade the shapes change  
[1:46:05] but you got the idea in the encoder we  uh in the encoder let me show you here
[1:46:17] here in the encoder we keep reducing the  size of the image until it becomes small  
[1:46:22] in the decoder we we need to return to the  original size of the image so we start with  
[1:46:28] the latent Dimension and we return to  the original dimension of the image
[1:46:38] convolution
[1:46:49] so we start with four channels and we output  four channels then we have another convolution
[1:47:01] we go to 500
[1:47:09] then we have a residual block just like before
[1:47:21] then we have an attention block
[1:47:27] then we have a bunch of resistor  blocks and we have four of them
[1:47:41] let me copy okay now the residual  blocks let me write some shapes here  
[1:47:49] here we arrived to a situation in which we have  batch size we have a 512 features and the size of  
[1:47:57] the image still didn't grow because we didn't  have any convolution that will make it grow
[1:48:07] this one of course will remain the  same because it's a residual block  
[1:48:11] and Etc now to increase the size of the images  so now the image is actually height divided  
[1:48:18] by 8 which height as you remember is 512 the  size of the image that we are working with so  
[1:48:24] this Dimension here is a 64 by 64. how can we  increase it we use one model called up sample
[1:48:37] the up sample we have to think of it like  um when we resize an image so imagine you  
[1:48:43] have an image that is 64 by 64 and you  want to transform into 128 by 128 the the  
[1:48:50] app sample will do it just like when we resize  an image so it will replicate the pixels will  
[1:48:56] um twice so along the dimensions right and down  for example twice so that the total amount of  
[1:49:04] pixels that the height and the width actually  doubles and um this is the same up sample  
[1:49:10] basically it will just replicate each pixel so  that by this scale factor along each dimension so
[1:49:21] this one becomes side but size
[1:49:27] divided by 8 with divide  by 8 becomes as we see here
[1:49:38] to divide by 4 and with divided by 4.
[1:49:44] then we have a convolution resistor  blocks so we have convolutions
[1:49:54] 2D
[1:49:57] 512 to 512
[1:50:05] then we have digital blocks of 512 500 but in  this case we have three of them two three then  
[1:50:12] we have another up sample that this will again  double the size of the image so we have another  
[1:50:19] one that will double the size of the image and by  a scale factor of two so now our image which was  
[1:50:26] divide by 4 with 512 channels so let's write  this like this will become divided by 2 now  
[1:50:35] so it will double the size of the  image so now our image is 256 by 256.
[1:50:44] then again we have a convolution
[1:50:52] and then we have three residual blocks again  but this time we reduce the number of features
[1:51:03] so 256
[1:51:08] and then it's 256 to 256.  
[1:51:12] okay then we have another up samplings which  will again double the size of the image
[1:51:20] and this time we will go from divide by  two to divide by 2 up to the original size
[1:51:30] and because the number of channels has  changed we are not 512 anymore okay and  
[1:51:38] then we have another convolution this case with  the 256 because it's the new number of features
[1:51:46] then we have another bunch of residual blocks  that will decrease the number of features
[1:51:56] so we go to 256 to 128
[1:52:06] we have finally a group normal
[1:52:10] 32 is the group sizes so we group features by in  groups of 32 before calculating the MU and the  
[1:52:18] sigma before normalizing and we said they find the  number of channels as 128 which is the number of  
[1:52:24] features that we have so this group normalization  will divide this 128 features into groups of 32.
[1:52:34] then we apply the silo
[1:52:39] and then we have a convolution  
[1:52:43] the final convolution that will transform  into an image with the three channels so RGB  
[1:52:49] by applying this convolutions here which  doesn't change the size of the output  
[1:52:55] so we'll go from an image that is batch size 128  yeah it width why height width because after the  
[1:53:04] last up sampling we become of the original  size into an image with only three channels
[1:53:18] and this is our decoder now we  can write the forward method
[1:53:36] I'm sorry if I'm putting a lot of spaces between  here but otherwise it's easy to get lost and not  
[1:53:42] understand where we are so here the input of  the decoder is our latent so it's batch size 4.  
[1:53:54] 8 divided by 8 with divide by 8. as you  remember here in the encoder the last  
[1:54:00] thing we do is we scale by this constant so we  nullify this scaling so we reverse this scaling  
[1:54:09] two one five and then we  run it through the decoder
[1:54:22] and then return X which is a  batch size 3 height and width  
[1:54:31] let me also write the input of  this decoder which is this one  
[1:54:36] we already have it okay this is our variational  Auto encoder so for so far let's go review
[1:54:46] we are building our architecture of the stable  diffusion so far we have built the encoder and  
[1:54:53] the decoder but now we have to build the unit and  then we have to build the clip text encoder and  
[1:55:01] finally we have to build the pipeline that will  connect all of these things so it's going to be  
[1:55:06] a long journey but this is it's fun actually  to build things because you learn every every  
[1:55:11] detail of how they work so the next thing that  we are going to build is the text encoder so  
[1:55:17] this is a clipping encoder here that will allow  us to encode The Prompt into embeddings that we  
[1:55:23] can then feed to this unit model here so let's  build this clip encoder and we will of course  
[1:55:29] use a pre-trained version so by downloading  the vocabulary and I will show you how it  
[1:55:35] works so let's start we go to visual studio code  we create a new file in SD folder called clip.pi  
[1:55:44] and here and we start importing the usual stuff
[1:56:08] and we also import self-attention because we will  using it so basically clip is a layer is very  
[1:56:16] similar to the encoder layer of the Transformer  so as you remember the Transformer let me show you  
[1:56:21] here the Transformer this is the encoder layer of  the Transformer is made of attention and then feed  
[1:56:29] forwards and there are many blocks like this one  after another that are applied one after another  
[1:56:33] we also have something that tells the position of  each token inside of the sentence and we will also  
[1:56:39] have something similar in clip so we need to build  something very similar to this one and actually  
[1:56:45] this is why I mean the the Transformer model was  very successful so that's why they use the same  
[1:56:49] structure of course also for this purpose and so  let's go to build it the first thing we will build  
[1:56:57] I will build first the skeleton of the model and  then we will build each block so let's build clip
[1:57:13] and this has some embeddings the embeddings allow  us to convert the tokens so as you remember in  
[1:57:21] when you have a sentence made up of text first  you convert it into numbers where each number  
[1:57:26] indicates the position of the token inside of the  vocabulary and then you convert it into embeddings  
[1:57:30] where each embedding represents a vector of size  512 in the original Transformer but here in clip  
[1:57:37] it's the size is 768 and each Vector represents  kind of the meaning of the word or the token  
[1:57:45] captures so this is an embedding and let's  redefine it we need the vocabulary size  
[1:57:52] the vocabulary size is 49408 I took it directly  from the file this is the embedding size and the  
[1:57:59] sequence length the maximum sequence length that  we can have because we need to use the padding is  
[1:58:04] the 77 because the we should use actually use  some configuration file to save butter because  
[1:58:11] we will be using with the pre-trained stable  diffusion model the size already fixed for us but  
[1:58:16] in the future I will refactor the code to add some  configuration actually to make it more extensible
[1:58:29] this is the list of layers which we'll call it  the clip layer we have this 12 which indicates the  
[1:58:38] number of head of the attention of the multi-head  attention and then the embedding size which is 768
[1:58:49] and we have 12 of these layers then  we have the layer normalization
[1:58:57] layer Norm
[1:59:01] can you tell him how many features so 768
[1:59:07] and then we Define the forward method
[1:59:13] this is
[1:59:16] a pencil and this one returns not  transfer why long tensor because  
[1:59:24] the I input IDs are usually numbers that  indicate the position of each token inside  
[1:59:29] of the vocabulary also this concept please if  it's not clear we'll go watch my previous video  
[1:59:34] about the Transformer because it's very clear  there when we work with the textual models
[1:59:45] okay first we convert each token into embeddings
[1:59:53] and then so what is the size here we are  going from batch size sequence length into  
[2:00:02] batch size sequence length and dimension where  the dimension is 768 then we apply one after uh  
[2:00:11] one after another all the layers of this encoder
[2:00:16] just like in the Transformer model
[2:00:21] and the last one we apply the layer normalization
[2:00:29] oh
[2:00:34] and finally we return the  output where the output is
[2:00:39] of course it's a simplest to sequence  model just like the transformer so the  
[2:00:43] input should match the size the shape of the  input should match the shape of the output  
[2:00:49] so we always obtain SQL select by  the model okay now let's define these  
[2:00:55] two blocks the first one is the clip  embedding so let's go clip embedding
[2:01:08] how much is the vocabulary  size what is the embedding size
[2:01:16] and uh number of token okay so  the sequence length basically
[2:01:28] and
[2:01:35] okay we Define the embedding itself using  an end dot embedding just like always
[2:01:47] we need to tell him what is the number of  embeddings or the vocabulary size and what is the  
[2:01:52] dimension of each factor of the embedding token  then we Define some positional encoding so now  
[2:01:59] as you remember the positional encoding  in the original Transformer are given by  
[2:02:02] sinusoidal functions but here in clip  they actually don't use them they use  
[2:02:08] some learned parameters so they have  these parameters that are learned  
[2:02:14] by the model during training that tell  the position of the token to the model
[2:02:23] applicants and and beddings like this
[2:02:38] we apply them so first we apply the embedding  so we go from patch size sequence length to  
[2:02:48] patch size so you can select dimension
[2:03:00] and then just like in the original Transformer we  
[2:03:03] add the positional encodings to the each position  to each token but in this case as I told you the  
[2:03:09] positional embeddings are not not fixed like not  a sinusoidal functions but they are learned by  
[2:03:15] the model so they are learned and then later we  will load these parameters when we load the model
[2:03:25] and then we return this X then we have  the clip layer which is just like the  
[2:03:30] layer of the Transformer model the  encoder of the Transformer model
[2:03:48] so returns nothing actually  and this one is wrong in it
[2:04:03] okay we have just like in the Transformer  block we have a um the pre Norm then we have  
[2:04:09] the attention then we have a post norm and then  we have the feed former so layer normalization
[2:04:26] can have the attention
[2:04:30] which is a self-attention
[2:04:35] later we will build the cross attention  and I will show you what is it
[2:04:42] and we have another layer normalization
[2:04:50] then we have two feed forward layers
[2:05:14] and finally we have the forward method finally
[2:05:21] so this one takes tensor under Transit  tensor so let me write it answer
[2:05:30] okay just like the transformer model okay let's  go have a look we have some a bunch of residual  
[2:05:37] connection as you can see here one residual  connection here one digital connection here  
[2:05:40] we have two normalization one here one here  the feed forward as just like in the original  
[2:05:45] Transformer we have two linear layers and  then we have this multi head attention which  
[2:05:51] is actually self-attention because it's the  same input that becomes query key and values  
[2:05:56] so let's do it the first residual connection  X so what is the input of this forward method  
[2:06:03] it's a batch size sequence length demo and  the dimension of the embedding which is 768.
[2:06:13] the first thing we do is we apply the  self attention but before applying this  
[2:06:17] of attention we apply the layer normalization  so layer number one then we apply the attention
[2:06:28] but with the causal mask
[2:06:33] uh as you remember here self-attention we  have the causal mask which basically means  
[2:06:40] that every token cannot watch the next tokens  so cannot be related to Future tokens but only  
[2:06:45] the one on the left of it and this is what  we want from a text model actually we don't  
[2:06:50] want the one word to watch the words that come  after it but only the words that come before it  
[2:06:57] then we do this residual connection so now  we are now we are doing this connection here
[2:07:06] then we do the feed forward layer  again we have a residual connection
[2:07:16] we apply the normalization
[2:07:22] I'm not writing all the shapes if you  watch my code online I have written all  
[2:07:26] of them but mostly to save time because  here the we are already familiar with  
[2:07:31] the structure of the Transformer hopefully  so I am not repeating all the shapes here
[2:07:39] yeah apply the first linear of the feed forward  them then as the as activation function we use  
[2:07:48] the glue function and that's what we call the  quick glow function which is defined like this  
[2:07:56] X multiplied by torch dot  sigmoid of 1.702 multiplied by X
[2:08:07] and that's it should be like this  
[2:08:12] so this is called the quick chilu activation  function also here there is no justification on  
[2:08:20] why we should use this one and not another one  they just saw that in practice this one works  
[2:08:25] better for this kind of application so that's  why we are using this function here so now
[2:08:35] and then we apply the residual connection  finally return X this is exactly like the  
[2:08:43] fifth forward layer of the Transformer except that  in the Transformer we don't have this activation  
[2:08:47] function but we have the relu function and if  you remember in llama we don't have the relu  
[2:08:52] function we have the zuiglo function but here we  are doing the quick glue function which I actually  
[2:08:57] know I'm not so familiar with but I I think that  it works good for this model and I just kept it  
[2:09:05] so now we have built our our text encoder here  clip which is very small as you can see and  
[2:09:11] our next thing to build is our unit so we have  built the variational route encoder the coder  
[2:09:19] part and the decoder part now the best and then  the next thing we have to build is this unit as  
[2:09:26] you remember the unit is the the network that will  give in some noisified image and the amount and  
[2:09:33] we also indicated the the to the network what is  the amount of noise that we added to this image  
[2:09:39] the model has to predict how much image  the noise is there and how to remove it  
[2:09:46] and this unit is a bunch of convolutions that  will reduce the size of the image as you can  
[2:09:52] see with each step by but by increasing the  number of features so we reduce the size but  
[2:09:59] we increase exactly what we did in the encoder of  the variational auto encoder and then we do the  
[2:10:06] reverse steps just like we did with the decoder  of the original encoder so now again we will work  
[2:10:12] with some convolutions with the residual blocks  with attentions Etc the one big difference is  
[2:10:18] that we need to tell our unit not only the image  that is already not in so how what is the image  
[2:10:26] with noise not only the amount of noises so the  time Step At which this noise was added but also  
[2:10:33] the prompt because as you remember we need to also  tell this um this unit what is our prompt because  
[2:10:40] we need to tell him how we want our output image  to be because there are many ways to denoise the  
[2:10:47] initial noise so if we want the initial noise to  become a dog we need to tell him we want a dog if  
[2:10:52] we want the initial noise to become a cat we need  to install him we want a cat so the unit has to  
[2:10:57] know what is the prompt and also he has to relate  this prompt with the rest of the information and  
[2:11:04] what is the best way to combine two different  stuff so for example an image with text we will  
[2:11:11] use what is called the cross attention cross  attention basically allows us to calculate the  
[2:11:16] attention between two sequences in which the  query is the first sequence and the keys and  
[2:11:23] the values are coming from another sequence so  let's go build it and let's see how this works
[2:11:30] now the first thing we will do is create  a new class new file here called diffusion  
[2:11:36] because this is will be our diffusion  model and I think also here I will build  
[2:11:44] um from top down so we first Define the diffusion  class and then we build each block one by one  
[2:11:51] let's start by importing the  usual libraries so import torch
[2:12:03] from perch
[2:12:13] and then we have import the attention
[2:12:18] self attention but also we will need the cross  attention and then shown a letter we will build it
[2:12:28] then we let's create the class diffusion
[2:12:32] the class division is basically our unit
[2:12:42] this is made of time embedding so something  that we will Define it later time embedding  
[2:12:53] 320 which is the size of the time embedding so  because um we need to give the unit not only the  
[2:13:00] noisified image but also the time Step At which it  was notified so the image the unit has needs some  
[2:13:06] way to to understand this time step so this is why  this time step which is a number will be converted  
[2:13:13] into an embedding by using this particular model  called that I'm embedding and later we will see it  
[2:13:20] then we build the unit
[2:13:24] and then the output layer of the unit and later  we will see what is it this output layer put layer  
[2:13:35] and we will see how to build  it let's do the forward
[2:13:43] as you remember the unit will receive the latent  so this Z which is the latent is the output of the  
[2:13:49] variational autoencoder so this latent which is  the torch dot tensor it will receive the context  
[2:13:55] what is the context is our prompt which is also  a torch dot tensor and it will receive the time  
[2:14:02] at which this latent was notified which is also  uh I don't remember I think it's a tensor also  
[2:14:11] later I Define it okay yeah it's tensor
[2:14:19] okay let's define the sizes so  the latent here is a batch size  
[2:14:24] 4 because 4 is the output of the encoder if  you remember correctly here four closing okay
[2:14:36] read and read divide by eight then we  have the context which is our prompt  
[2:14:43] which we already converted with the clip encoder  here which will be a bit size by sequence length  
[2:14:50] by Dimension where the dimension is 768 like  we defined before and the time would be another  
[2:14:56] um we will Define it later how it's defined  how it's built but it's each embedding but  
[2:15:03] it's a number with an embedding of  size it's a vector of a size of 320.
[2:15:10] the first thing we do is we  convert this time into an embedding
[2:15:15] and actually this time we will  see later that it's actually  
[2:15:19] um just like the positional encoding of the  Transformer model it's actually a number that  
[2:15:26] is multiplied by signs and cosines just like  in the Transformer because it's a they saw  
[2:15:32] that it works for the Transformer so they we  can also use the same positional encoding to  
[2:15:36] convey the information of the time which  is actually kind of an information about  
[2:15:40] position so it tells the model at which  step we arrived in the denoisification
[2:15:48] so this one will convert tensor of 1 320 into  a tensor of one one two eight zero one thousand
[2:16:00] the unit will convert our latent into another  latent so it will not um change the size
[2:16:11] batch
[2:16:14] for height
[2:16:18] this is the output of the variational decoder  which first becomes batch 320 features
[2:16:30] through this the unit so
[2:16:42] so why here we have more features than the  starting because let's review here as you  
[2:16:48] can see the last layer of the unit actually  then we need to go back to the same number of  
[2:16:55] um the features you can see here so here we start  actually the dimensions here don't match what we  
[2:17:02] will be using so this is the original unit but  the one used by stable diffusion is a modified  
[2:17:07] unit so in the last one we built the decoder  the decoder will not build the final number  
[2:17:14] of features that we need which is four but we  need an additional output layer to go back to  
[2:17:19] the original size of features and this is the job  of this output layer so later we will see when we  
[2:17:26] build this this layer so output is equal to self  dot final this one will go from this size here
[2:17:39] to back to this original size of the unit  because the unit his job is to take in  
[2:17:47] latents predict how much noise is it then take  again the same latent predict how much noise  
[2:17:53] we remove it we remove the noise then again we  give another latent we predict how much noise  
[2:17:58] we remove the noise we give another latent we  predict the noise we remove the noise etc etc  
[2:18:03] etc so the output Dimension must match the input  dimension and then we return the output which is  
[2:18:12] the latent like this let's build the first  the time embedding I think it's easy to build  
[2:18:21] so something that encodes information  about the time step in which we are
[2:18:38] okay
[2:18:44] it is made of two linear layers  so nothing fancy here the nut one
[2:18:54] which will map it to four by and  embedding and then linear two
[2:19:05] four by and embedding into  
[2:19:09] buy and embedding and now you understand  why it becomes a 1280 which is 4 times 320.
[2:19:25] this one returns to tensor  so the input size is 1 320.  
[2:19:34] what we do is first we apply this first layer  linear one then we apply The Silo function
[2:19:44] then we apply again the second linear layer
[2:19:49] and then we'll turn it nothing special here  the last the output Dimension is 1 by 1280.
[2:20:01] 280 okay the next thing we need to build is the  unit the unit will require many blocks so let's  
[2:20:12] first build the unit itself and then we build each  of the blocks that it will require so class unit
[2:20:36] as you can see the unit is made up of one encoder  Branch so this this is like the encoder of the  
[2:20:42] variation route encoder so things go down so  become the image becomes smaller smaller smaller  
[2:20:47] but the channels keep increasing the features  keep increasing then we have this bottleneck  
[2:20:52] layer here it's called the bottleneck and then we  have a decoder part here so it becomes original  
[2:20:58] size the image from the very small size becomes  the original size and then we have this skip  
[2:21:03] connections between the encoder and the decoder  so the output of each layer of each layer of which  
[2:21:10] step of the encoder is connected to the same step  of the decoder on the other side and you will see  
[2:21:18] this one here so we start building the left side  which is the encoders which is a list of models
[2:21:27] and to build these encoders we need  to define a special um a special um  
[2:21:36] layer basically that will apply okay let's build  it and then I will describe it switch sequential
[2:21:58] and basically this sequence is a switch sequential  given a list of layers we'll apply them one by one  
[2:22:05] so we can think of it as a sequential but it can  recognize what are the parameters of each of them  
[2:22:12] and we'll apply accordingly so after I Define it  it will be more clear so first we have just like  
[2:22:19] before a convolution because we want to increase  the number of channels so as you can see at the  
[2:22:24] beginning we increase the number of channels of  the image here it's 64 but we go directly to 320.  
[2:22:31] and then we have another one of these switch  sequentials which is a unit residual block  
[2:22:43] we Define it later but it's very similar to the  residual block that we built already for the  
[2:22:48] operational autoencoder and then we  have an attention block which is also  
[2:22:52] very similar to the attention block that  we built for the evolution Auto encoder
[2:23:03] then we have a okay I think it's better to  build this switch sequential otherwise we  
[2:23:08] have too many yeah let's build it it's very simple
[2:23:19] as you can see it's a sequence
[2:23:24] but given X so which is our latent
[2:23:29] which is the third top sensor our context so  our prompt and the time which is also a tensor
[2:23:41] we'll apply them one by one
[2:23:47] but based on what they are so if the layer  is a unit attention block for example
[2:23:55] it will apply it like this so layer of  X and context why because this attention  
[2:24:01] block basically will compute the cross attention  between our latents and the prompt this is why
[2:24:15] this residual block will compute um  
[2:24:19] will match our latent with its time step and  then if it's any other layer we just apply it
[2:24:31] and then we return but after the full while yeah  
[2:24:35] so this is now in the CDC we just need to Define  this residual block and this attention block
[2:24:44] then we have another sequence  sequential switch this one here
[2:24:51] so the code I'm writing actually is based on  a repository uh or upon which actually most  
[2:24:57] of the code I rotor is based on which is in turn  based on another repository which was originally  
[2:25:02] written for tensorflow if I remember correctly  so actually the the code for stable diffusion  
[2:25:07] it uh because it's a model that we is built by  completes group at the LMU University of course  
[2:25:14] it cannot be different from that code so most of  the code are actually similar to each other I mean  
[2:25:19] you cannot create the same model and change  the code of course the code will be similar  
[2:25:25] so we again use this one which sequential so here  
[2:25:33] we are building the encoder side so  we are reducing the size of the image
[2:25:54] let me check where we are so we have  the resistor block of 320 to 64.  
[2:26:02] and then we have an attention block of 8 to 80.  and this attention block takes the number of Heads  
[2:26:10] This 8 indicates the number of head and this  indicates the embedding size we will see later  
[2:26:15] how we transform this the output of this into  a sequence so that we can run attention on it  
[2:26:24] um okay we have this sequential  and then we have another one
[2:26:36] then we have another convolution
[2:26:41] let me just copy convolution of size from 6 640  to 640 channels kernel size 3 stride 2 padding one
[2:26:54] then we have another residual block
[2:26:57] that will again increase the  features so from 640 to 1280
[2:27:08] and then we have an attention block of  8 heads and 160 is the embedding size  
[2:27:15] then we have another digital block of 1 to 80  and 1 to 80 and 8 and 160. so as you can see  
[2:27:29] just like in the encoder of the variational  encoder we with these convolutions we keep  
[2:27:36] decreasing the size of the image so actually here  we started with the latent representation which  
[2:27:43] was a height divided by 8 and height divided by  eight so let me write some shapes here at least  
[2:27:49] you need to understand this size changes so batch  size 4 height divided by 8 and width divide by 8  
[2:28:00] when we apply this convolution  it will become divided by 16 so
[2:28:11] it will become divided by 16
[2:28:15] so it will become a very small image and after we  apply the second one it will become divided by 32  
[2:28:24] so here we start from 16 here it will become  divided by 32 so what does it mean divided  
[2:28:33] by 32 that if the initial image was of  size 512 the latent is of size 64 by 64.  
[2:28:41] then it becomes a 32 by 32 now it has become 16  by 16. and then we apply this residual connections
[2:28:54] and then we apply another convolutional layer  which will reduce the size of the image further
[2:29:04] from 32 here divide by 32 and divide by 32  to divide by 64. every time we divided the we  
[2:29:16] divided the size of the image by two
[2:29:20] and the number of features we want  to eight zero one two eight zero
[2:29:31] then we have a unit this is your  block so let me copy also this one
[2:29:42] of 1 to 80 and 1380 and then we have a last  one which is another one of the same size  
[2:29:51] so now we have an image that is 64 divided by 64  and divide by 64 but with much more channels I  
[2:30:00] forgot to change the channel numbers here so here  is 1280 channels and divide by 64 divided by 64s  
[2:30:07] and this one Remains the Same because the  residual connections don't change the size  
[2:30:13] here should be 1280.
[2:30:18] to 1280 here should be 640 to  640 and here it should be 320.  
[2:30:30] to 320. so as I said before we keep reducing  the size of the image but we keep increasing  
[2:30:37] this number of features of each pixel  basically then we build the bottleneck
[2:30:45] which is this part here of the unit
[2:30:55] this is a sequence of a residual block
[2:31:05] then we have the attention block
[2:31:09] which will make self-attention  sorry not serve the cross attention  
[2:31:14] and then we have another residual block
[2:31:22] then we have the decoder so in the decoder  we will do the opposite of what we did in  
[2:31:27] the encoder so we will reduce the number  of features but increase the image size
[2:31:40] again let's start with our  beautiful switch sequential
[2:31:49] so we have a 2560 to 1280. y here is what 2560  even if after the bottleneck we have at 1280 so  
[2:32:03] we are talking about this part here so after the  the input the of the decoder so this side here  
[2:32:12] of the unit is the output of the bottleneck but  the bottleneck is outputting 1280 features while  
[2:32:20] the encoder is expecting 2560 so double the amount  why because we need to consider that we have this  
[2:32:27] skip connection here so this skip connection  will double the amount at each layer here  
[2:32:33] and this is why the input we expect here  is double the size of what is the output  
[2:32:37] of the previous layer let me write  some shapes also here so back size  
[2:32:42] 2560 the image is very small so height part  and read divide by 64. and it will become
[2:32:58] individual okay
[2:33:02] then we apply another switch sequential
[2:33:07] of the same size then we apply another one  with an up sample just like we did in the  
[2:33:13] traditional encoder so if you remember in the  additional encoder to increase the size of  
[2:33:18] the image we do up sampling and this is  what we do exactly here we do up sample  
[2:33:25] but this is not the up sample that we did exactly  the same but it's this the concept is similar and
[2:33:34] we will Define it later also this one
[2:33:42] so we have another residual with  attention so we have a residual of 2000  
[2:33:49] and then we have an attention block
[2:33:55] 8 by 160 then we have again this one
[2:34:05] then we have another one with  up sampling so we have 90 20
[2:34:18] and then we have an up sample
[2:34:26] so this one is small so I know that I'm  not writing all the shapes but otherwise  
[2:34:33] it's really a tiring job and very long so just  remember that we are keep increasing the size  
[2:34:40] of the image but we we will decrease the number  of features later we will see that this number  
[2:34:45] here will become very small and the size of  the image will become nearly to the normal
[2:34:54] then we have another one with attention
[2:35:03] so as you can see we are  decreasing the features here  
[2:35:06] then we have 8 by 80 and we are increasing  also here the size and then we have another one
[2:35:22] and 880. then we have another one with up  sampling so we increase the size of the image
[2:35:30] so 960
[2:35:33] 640 8 heads with the dimensions betting size  of 80 and the up sampling with 640 features  
[2:35:44] and then we have another  residual block with attention
[2:35:58] 40
[2:36:01] then we have another one which is a 600 40  320 840 and finally the last one we have 640
[2:36:15] by 320 and 8 and 40. this Dimension here  is the same that will be applied by the  
[2:36:26] uh there is the output of  the unit as you can see here  
[2:36:30] this one here and then we will give it to the  final layer to build the original latent size
[2:36:37] okay let's build all these blocks that we didn't  build before so first let's build the up sample  
[2:36:45] let's build it here
[2:36:49] which is exactly the same as the true okay
[2:37:08] you have this convolution
[2:37:16] without changing the number of features
[2:37:20] and this is also doesn't change  the size of the image actually
[2:37:33] so we will go from batch channels or features  let's call it features height width to
[2:37:44] batch size features height multiplied by  two and width multiplied by 2 y because  
[2:37:52] we are going to use the up sampling  this interpolation that we will do now
[2:38:00] X scale factor equal to mode is equal to  nearest is the same operation that we did here  
[2:38:09] the same operation here it  will double the size basically
[2:38:16] and then we apply a convolution
[2:38:20] now um we have we have to define the final block  
[2:38:26] and we also have to Define so the output layer  and we also have to define the attention block  
[2:38:32] and the residual block so let's build first  this output layer it's easier to build
[2:39:02] so that's this one also has a group normalization  again with the 32 size of the group 32
[2:39:10] also has a convolution
[2:39:19] and a padding of one
[2:39:25] okay the final layer needs to convert this  shape into this shape so 320 features into  
[2:39:33] four we have a so we have a input which  is back size of 320 features the height  
[2:39:42] is divided by 8 and the width is divided by  eight we first apply a group normalization
[2:39:53] then we apply the silu
[2:39:58] then we apply the convolution and then we return  this will basically the convolution let me write  
[2:40:06] also why we are reducing the size this convolution  will change the number of channels from in to out  
[2:40:13] and when we will declare it we say that we want  to convert from 320 to 4 here so this one will be  
[2:40:19] of say of shape batch size 4. height divided by 4  I divided by 4 and width divide by 4 divide by 8.
[2:40:33] then we need to go build this residual block  and this attention attention block here so  
[2:40:40] let's build it here let's start with the  residual block which is very similar to  
[2:40:45] the residual block that we built for the  variational autoencoder so tune it lock
[2:41:11] this is the embedding of the time the the  time the time step as you remember we put  
[2:41:18] the time embedding we transform into  an embedding of size of one to 1280.
[2:41:30] we have this group normalization  it's always this group Norm
[2:41:41] then we have a convolution
[2:41:57] and we have a linear for the time embedding
[2:42:09] then we have another group of normalization  we will see later what is this merged
[2:42:24] and another convolution
[2:42:34] oops kernel size three typing one again  just like before we have if the in channels  
[2:42:44] is equal to the old channels we can connect  them directly with the resistor connection
[2:42:54] otherwise we create a convolution to  connect them to convert the size of the  
[2:42:58] input into the output otherwise  we cannot add the two tensors
[2:43:18] zero okay
[2:43:30] yeah so it takes in as input this feature  tensor which is actually the latent  
[2:43:39] batch size in channels then we have hate and  width and then also the time embedding which is  
[2:43:47] one by one two eight zero just like here and  we build first of all a resistor connection
[2:43:59] then we do apply the group normalization so  the usually there is a lot of connection the  
[2:44:03] result of blocks are more or less always  the same so there is a normalization an  
[2:44:07] activation function then we can  have some skip connection etc etc
[2:44:37] then we have the time
[2:44:59] here we are merging the latents with the  time embedding but the time embedding  
[2:45:04] doesn't have the batch and the channels  Dimension so we add it here with unsqueeze  
[2:45:11] and we merge them then we  normalize this merged connection
[2:45:18] it's why it's called merged we  apply the activation function
[2:45:34] then we apply this convolution and  finally we apply the residual connection
[2:45:47] so why are we doing this well the idea is  that here we have three input we have the  
[2:45:53] time embedding we have the latent we have the  prompt we need to find a way to combine the  
[2:45:59] three information together so the unit needs  to learn to detect the noise present in the  
[2:46:04] noisified image at a particular time step using  a particular prompt as a condition which means  
[2:46:10] that the model needs to recognize this this time  embedding and needs to relate this time embedding  
[2:46:17] with the latency and this is exactly what we are  doing in this resistor block here we are relating  
[2:46:22] the latent with the time embedding so that the  output will depend on the combination of both  
[2:46:29] not on the single noise or in the single time step  and this will also be done with the context using  
[2:46:35] cross attention in the attention block that we  will build now so neural net attention block
[2:46:58] 768 okay
[2:47:13] okay I will Define some layers that  for now will not make much sense  
[2:47:18] but later they will make sense  when we make the forward method
[2:47:56] so my cat is asking for food I think he already  have food but maybe he wants to eat something  
[2:48:05] special today so let me finish this  attention block and the unit and then all his
[2:48:14] why everyone wants attention  self-attention and head channels  
[2:48:23] here we don't have any buyers as you remember  the self attention we can have the buyers  
[2:48:27] for the W matrices here we don't have any  bias just like in the vanilla Transformer  
[2:48:33] so we have this attention then we have a  layer normalization self-taught layer Norm two
[2:48:43] which is along the same number of  features then we have another attention  
[2:48:47] we will see later why we need all  this attention but this is not a  
[2:48:50] self attention it's a cross attention  and we will see later how it works
[2:49:03] then we have the layer Norm tree
[2:49:29] this is because we are using a  function that is called the check  
[2:49:32] activation function so we need these matrices here
[2:49:58] okay now we can build the forward method so our X  is our latency so we have a batch size we have a  
[2:50:06] feature so we have height we have a width then  we have our context which is our prompt which  
[2:50:12] is a batch size sequence length Dimension  the dimension is size 768 as we saw before  
[2:50:20] so the first thing we will do is we will do the  normalization so just like in the Transformer  
[2:50:26] we will take the input so our latents and we  apply the normalization and the convolution  
[2:50:32] in actually in the Transformer there is  no convolution but only the normalization
[2:50:38] so residue this is called the long residual  because it will be applied at the end
[2:50:49] okay so we have this we are applying  the normalization which doesn't change  
[2:50:53] the size of the tensor then we have a convolution
[2:50:58] X is equal to self.com input of X which  also doesn't change the size of the tensor  
[2:51:05] then we have a we take the shape
[2:51:12] which is the size of the the batch size the  number of features the height and the width  
[2:51:19] we transpose because we want  to apply cross attention  
[2:51:23] first we apply self attention  then we apply cross attention so  
[2:51:28] so we do normalization Plus self attention  up s itself attention with skip connection
[2:51:42] so X is the x dot transpose of minus  1 minus 2 so we are going from this
[2:51:52] uh wait I forgot something here first of  all we need to do X is equal to x dot U  
[2:51:59] and C H multiplied by W so  we are going from this to
[2:52:09] um bed size features and then H multiplied  by W so this one multiplied by this  
[2:52:15] then we transpose these two  Dimensions so now we get from here
[2:52:21] to here so the features become the last one now  we apply this normalization plus cell potential so  
[2:52:30] and so we have a first short residual connection  that will apply right after the attention so we  
[2:52:38] say that X is equal to layer Norm 1 so X then  we apply the attention so self dot attention one  
[2:52:47] and then we apply the residual connection so X  is plus equal to residual sure short the first  
[2:52:54] residual connection then we say that the residual  short is again equal to 6 because we are doing to  
[2:53:00] apply now the cross attention so now we apply the  normalization cross attention with skip connection
[2:53:16] so what we did here is what we do in any  Transformer so let me show you here what  
[2:53:21] we do in any Transformer so we apply some  normalization we calculate the attention  
[2:53:25] and then we combine it with the skip connection  here and now we will instead of translating a self  
[2:53:31] attention we will do a cross attention which we  still didn't defined we will Define it later so  
[2:53:38] short and then first we calculate  we apply the normalization
[2:53:48] then the cross attention between the latent  and the prompt this is the cross attention  
[2:53:54] so this is cross attention and we will see  how and X Plus or equal to residual short
[2:54:08] okay and then again equal to X  finally just like with the attention  
[2:54:15] um the Transformer we have a feed forward  layer with the J glue activation function
[2:54:41] and this is actually if you watch  the original implementation of the  
[2:54:44] Transformer of the stable division  it's implemented exactly like this  
[2:54:48] so it's uh basically later we do a mult  um we do element wise multiplication
[2:54:59] so these are special activation functions  that involve a lot of parameters
[2:55:07] but why we use one and not the other  I told you just like before they just  
[2:55:12] saw that this one works better for this  kind of application there is no other
[2:55:21] then we apply the skip connection so  we apply the cross attention then we  
[2:55:25] Define another one here so this  one is basically normalization
[2:55:32] Plus forward layer with J  glue and script connection  
[2:55:39] in which the script connection is defined here so  at the end we always apply the script connection  
[2:55:44] finally we change back to our tensor to  not be a sequence of pixels anymore so
[2:55:52] we reverse the previous transposition
[2:55:57] transpose
[2:56:00] two so basically we go from batch size  
[2:56:06] with multiplied by eight eight multiplied by  width and features into batch size features
[2:56:22] eight multiplied by width then we remove this  multiplication so we reverse this multiplication
[2:56:35] and chw
[2:56:39] finally we will apply the long skip connection  that we defined here at the beginning so  
[2:56:45] only if the size match if the sizes don't  match we apply the here this one we have here
[2:56:53] turn self.com output
[2:57:01] and this is all of our unit we have defined  everything I think except for the cross  
[2:57:08] attention which is very fast so we go to the  attention that we defined before and we I put  
[2:57:14] it in the wrong folder it should be script  changes let me check if I put it correctly
[2:57:23] yeah we only need to Define this cross  attention here okay attention so let's go  
[2:57:34] and let's define this cross attention so class  it will be very similar to the not very similar  
[2:57:43] actually same as the self-attention except that  the keys come from one side and the query and  
[2:57:50] the sorry the query come from one side and  the key and the values from Another Side
[2:58:11] this is the dimension of the embedding of the  keys and the values this is the one of the queries
[2:58:35] this is the WQ Matrix in this case we will  Define the instead of one big Matrix made  
[2:58:41] of three wqw K and WV will Define three  different matrices both systems are fine  
[2:58:47] you can Define it as one big mattress or three  separately it doesn't change anything actually
[2:59:10] so the cross is from the the keys and the values
[2:59:42] oops linear
[2:59:54] then we save the number of heads of this cross  attention and also the dimension of each how much  
[3:00:02] information each head will see and oops d head is  equal to the embed divided by the number of heads  
[3:00:15] let's define the forward method X is  our query and Y is our keys and values
[3:00:27] so we are relating X which is our  latents which is of size batch size  
[3:00:37] it will have a sequence length its own sequence  length to Q let's call it q and its own dimension  
[3:00:45] and the Y which is the context or  the prompt which will be batch size
[3:00:53] sequence length of the Kiwi because the  prompt is will become the key and the  
[3:00:58] values and each of them will have its  own embedding size the dimension of KV  
[3:01:04] we can already say that this will  be a batch size of 77 because our  
[3:01:08] sequence length of the prompt is  77 and it embedding is of size 768.
[3:01:17] so let's build this one this is  input shape is equal to export shape
[3:01:36] okay then we have the interim  shape like the same as before so
[3:01:46] this is the sequence length then the N number of  heads and how much information each head will see  
[3:01:54] the head
[3:02:00] the first thing we do is multiply  queries by WQ Matrix so query is equal to
[3:02:10] then we do the same for the keys and the  values but by using the other matrices
[3:02:18] and as I told you before the key and  the values are the Y and not the X
[3:02:28] again we split them into H  heads so H number of heads
[3:02:37] very transpose I will not write  the shifts because they're they  
[3:02:42] match the same transformation that we do here
[3:03:01] okay again we calculate the  
[3:03:05] weight as which is the attention as a query  multiplied by the transpose of the keys
[3:03:15] and then we divide it by the dimension  of each head or the square root
[3:03:27] then we do the soft Max in this case we don't have  any causal mask so we don't need to apply the mask  
[3:03:35] like before because here we are trying to relate  the tokens so that the prompt with the pixels so  
[3:03:43] the each pixel can watch any word of the token  and every any token can watch any pixel basically
[3:03:55] so we don't need any mask
[3:04:00] we are to obtain the object  we multiplied by the V Matrix  
[3:04:06] and then the output again is  transposed just like before  
[3:04:11] so now we are doing exactly the same things  that we did here so transpose reshape Etc
[3:04:37] and then return output and this ends  our building of the let me show you  
[3:04:43] now we have built all the building blocks for  the stable diffusion so now we can finally  
[3:04:52] combine them together so the next thing that  we are going to do is to create the system that  
[3:04:57] taking the noise taking the text taking the time  embedding will run for example if we want to do  
[3:05:04] text to image we'll run this noise many times  through the unit according to a schedule so we  
[3:05:11] will build the scheduler which means that because  the unit is trained to predict how much noise is  
[3:05:17] there but we then need to remove this noise so to  go from a noisy version to go to to obtain a less  
[3:05:24] noisy version we need to remove the noise that is  predicted by the unit and this job is done by the  
[3:05:29] scheduler and now we will build the scheduler we  will build the decoder to load the weights of the  
[3:05:35] pre-trained model and then we combine all these  things together and we actually built the what is  
[3:05:40] called the pipeline so the pipeline of text to  image image to image Etc and let's go now that  
[3:05:49] we have built all the the structure of the unit  or we have built the variational auto encoder we  
[3:05:54] have built a clip we have built the attention  blocks Etc now we it's time to combine all it  
[3:06:01] all together so the first thing I kindly ask you  to do is to actually download the pre-trained  
[3:06:06] weights of the stable division because we need to  inference it later so if you go to the repository  
[3:06:11] I shared this one python stable diffusion you can  download the the pre-trained weights of the stable  
[3:06:17] diffusion 1.5 directly from the website of hugging  face so you download this file here which is the  
[3:06:24] EMA which means exponentially moving average  which means that it's it's a model that has been  
[3:06:30] trained but they didn't change the weights at each  iteration but with an exponentially moving average  
[3:06:34] schedule so this is good for inferencing it means  that the weights are more stable but if you want  
[3:06:40] to fine tune later the model you need to download  this one and we also need to download the files  
[3:06:47] of the tokenizer because we of course we will give  some prompt to the model to generate an image and  
[3:06:54] the prompt needs to be tokenized by a tokenizer  which will convert the words into tokens and  
[3:06:59] the tokens into numbers the numbers will then be  mapped into embeddings by our clip embedding here  
[3:07:06] so we need to download two files for the tokenizer  so first of all the weights of the this one file  
[3:07:12] here then when they tokenize the folder we find  the merges.txt and devocab.json if we look at the  
[3:07:18] vocab.json file which I already downloaded it's  basically a vocabulary so each token mapped to a  
[3:07:25] number that's it just like what the tokenizer does  and then I also prepared the picture of a dog that  
[3:07:30] I will be using for image to image but you can use  any image you don't have to use the one I am using  
[3:07:36] of course so now let's first build the pipeline  so how we will inference this stable diffusion  
[3:07:44] model and then while building the pipeline I will  also explain you how the scheduler will work and  
[3:07:52] we will build the scheduler later I will explain  all the formulas all the mathematics behind it  
[3:07:58] so let's start let's create a new file let's call  it pipeline dot pi and we import the usual stuff
[3:08:11] numpy Ops empty
[3:08:18] we will also use a tqdm to show the progress  bar and later we will build this sampler the  
[3:08:27] ddpm sampler and we will build it later and I will  also explain what is this sampler doing and how it  
[3:08:34] works etc etc so first of all let's define some  constants we the stable diffusion can only accept  
[3:08:40] the produce images of size 512 by 512 so height  is 500 by 512. the latent size latent Dimension  
[3:08:50] is the the size of the latent tensor of the  variational autoencoder and as we saw before  
[3:08:57] if we go check the size the encoder of the  variational encoder will convert something that  
[3:09:04] is 512 by 512 into something that is 512 divided  by eight so the latent Dimension is 512 divided by  
[3:09:12] 8 and the same goes on for the height 512 divided  by 8 we can also call it width divided by 8 and 8  
[3:09:22] divided by 8. then we create a function called the  generator this will be the main function that will  
[3:09:29] be allow us to do text to image and also image to  image which accepts a parameter which is a string  
[3:09:38] and unconditional prompt so unconditional prompt  this is also called the negative prompt if you  
[3:09:45] ever used stable diffusion for example with the  hugging phase Library you will know that there  
[3:09:49] is a you can also specify a negative prompt  which tells that uh you want for example you  
[3:09:55] want a picture of a cat but you don't want  the cat to be on the sofa so for example you  
[3:10:01] can put the word sofa in the negative prompt so  it will try to go away from the concept of sofa  
[3:10:07] when generating the image something like this  and this is connected with the classifier free  
[3:10:12] guidance that we saw before so but don't worry I  will repeat all the concepts while we are building  
[3:10:16] it so this is also a string we can have an input  image in case we are building an image to image  
[3:10:25] and then we have the strength strength I will  show you later what is it but it's related to  
[3:10:31] if we have an input image and how much if we start  from an image to generate another image how much  
[3:10:37] attention we want to pay to the initial starting  image and we can also have a parameter called do  
[3:10:45] CFG which means do classifier free guidance we  set it to yes CFG scale which is the weight of  
[3:10:53] how much we want the model to pay attention to  our prompt it's a value that goes from 1 to 14  
[3:10:58] we start with 7.5 the sampler name we will  only Implement one so it's called adpm how  
[3:11:07] many inference steps we want to do and we will do  50 I think it's quite common to do 50 steps which  
[3:11:15] produces actually not bad results the models are  the pre-trained models the seed is how we want to  
[3:11:22] initialize our random number generator let me put  a new line otherwise we become crazy reading this
[3:11:31] okay new line so seed then we have the  device where we want to create our tensor  
[3:11:40] we have an idle device which means basically  if we load some model on Cuda and then we don't  
[3:11:45] need the model we move it to the CPU and  then the tokenizer that we will load later  
[3:11:51] recognizer is not okay this is our method this is  our main pipeline that given all this information  
[3:11:57] will generate one picture so it will pay attention  to the prompter it will pay attention to the input  
[3:12:02] image if there is according to the weights that  we have specified so the strength and the CFG  
[3:12:08] scale I will repeat all this Concepts don't  worry later I will explain them actually how  
[3:12:13] they work also on the code level so let's start  so the first thing we do is um we disable okay  
[3:12:24] torch Dot dot no grad because  we are inferencing the model  
[3:12:29] the first thing we make sure is the strength  should be uh between zero and one so if
[3:12:42] then we raise an error where is value error
[3:12:50] must be
[3:12:52] between zero and one
[3:12:57] this Idol device  
[3:13:00] it was basically if we want to move things  to the CPU we create this Lambda function
[3:13:16] otherwise
[3:13:25] okay then we create the oops I think I okay  then we created a random number generator  
[3:13:37] that we will use I think I made some mess  with this this one should be like here
[3:13:50] okay and the generator is a random number  generator that we will use to generate the noise  
[3:14:01] and if we the we we want to  start it with the seed so if seed
[3:14:07] then we generate with the condom seed  otherwise we specify one manually
[3:14:22] let me fix this formatting because I don't  know format document okay now at least today
[3:14:32] then we Define clip the clip is a model that  we take from the pre-trained models so it will  
[3:14:38] have the clip model inside so this model here  basically this one here we move it to our device
[3:14:56] okay as you remember with the classifier  free guidance so let me go back to my slides
[3:15:08] when we do classifier free guidance we inference  the model twice first with by specifying the  
[3:15:15] conditions for the prompt and another time  by not specifying the condition so without  
[3:15:19] the prompt and then we combine the output of the  model linearly with a weight this weight w is our  
[3:15:29] this weight here CFG scale it indicates how much  we want to pay attention to the conditioned output  
[3:15:36] with respect to the unconditional output which  also means that how much we want the model to  
[3:15:40] pay attention to the condition that we have  specified what is the condition The Prompt  
[3:15:45] the textual prompt that we have written and and  the unconditioned actually is also will use the  
[3:15:54] negative prompt so the negative prompt that you  use in stable diffusion which is this parameter  
[3:15:59] here so unconditioned prompt this is the  unconditional output so we we will sample  
[3:16:05] the we will inference from the model twice one  with the prompt one without with the one with the  
[3:16:11] prompter one with the unconditioned prompt which  is usually an empty text an empty string and then  
[3:16:18] we combine the two by this and this will tell the  model by using this weight we will combine the  
[3:16:22] output in such a way that we can decide how much  we want the model to pay attention to the prompter  
[3:16:29] so let's do it if we want to do classifier for  guidance first convert The Prompt into tokens
[3:16:40] using the tokenizer
[3:16:45] we didn't specify what is the tokenizer  lens yet but later we will define it  
[3:16:50] so the conditional tokens tokenizer batch  
[3:16:57] code Plus we want to encode the prompter we want  to append the padding up to the maximum length  
[3:17:07] which means that the prompt if it's too short  it will fill up it with the paddings and the  
[3:17:12] max length as you remember is 77 because we have  also defined it here the sequence length is 77
[3:17:21] and we take the input IDs of this tokenizer  then we convert this this tokens which are input  
[3:17:31] IDs into a tensor which will be of size batch  size and sequence length so conditional tokens
[3:17:52] let me put it in the right device now we  run it through clip so it will convert the  
[3:17:58] batch size sequence length so this input  is easy will be converted into embeddings
[3:18:05] of size 768 each Vector of  size 768 so let's call it dim  
[3:18:14] and what we do is conditional context is equal  to clip of conditional tokens so we are taking  
[3:18:23] these tokens and we are running them through  Clips so this forward method here which will  
[3:18:28] return batch size sequence length Dimension  and this is exactly what I have written here  
[3:18:34] we do the same for the unconditioned tokens  so the negative prompt which if you don't  
[3:18:39] want to specify we will use the empty string  which means the unconditional output of the  
[3:18:43] model so the uh the model the what would the  model um produce without any condition so if  
[3:18:52] we start with random noise and we ask the model  to produce an image it will produce an image but  
[3:18:56] without any condition so the model will output  anything that it wants based on the initial noise
[3:19:27] we've convert it into tensor then we pass it  through Clips just like the conditional tokens  
[3:19:34] so they will it will become tokens yes  
[3:19:40] so it will also become a tensor of the size but  size sequence length Dimension where the sequence  
[3:19:48] length is actually always 77 and also in this case  it was always 77 because it's the max length here  
[3:19:55] but I forgot to write the code to convert it into  so unconditional tokens is equal izer batch Plus
[3:20:10] so the unconditional prompt so also the negative  front the padding is the same as before so max  
[3:20:19] length and the max length is defined as 77. and we  take the input IDs from here so now we have these  
[3:20:29] two prompts what we do is we concatenate them they  will become the batch of our input to the unit
[3:20:48] okay so basically what we are doing is we  are taking the conditional and unconditional  
[3:20:53] input and we are combining them into  one single tensor so they will become  
[3:20:58] a tensor of size batch size too so  two sequence length and dimension  
[3:21:06] where sequence length is actually we can already  write it it will become 2 by 77 by 768 because  
[3:21:13] 77 is the sequence length and the dimension  is 768. if we don't want to do a conditional  
[3:21:21] um classifier classified free guidance we  only need to use the prompt and that's it  
[3:21:27] so we just we do only one step through the unit  and only with the prompt without combining the  
[3:21:35] unconditional input with the conditional input  but in this case we cannot decide how much the  
[3:21:41] model pays attention to the the the prompt  because we don't have anything to combine it with
[3:21:55] so again we take the just  a prompt just like before
[3:22:01] you can take it let's call it just tokens
[3:22:06] and then we transform this into a tensor
[3:22:13] okay so long we put it in the right device
[3:22:21] we calculate the context which is  one big tensor we pass it through  
[3:22:26] clip but this case it will be only one only one so  the batch size will be one so the batch Dimension  
[3:22:35] the sequence is again 77 and the dimension is  768. so here we are combining two prompts here  
[3:22:41] we are combining one why because we will run  through the model two prompts one unconditioned  
[3:22:47] one conditioned so one with the prompt that we  want one with the empty string and the model will  
[3:22:53] produce two output because the models take care of  the batch size that's why we have the batch size
[3:23:00] since we have finished using the clip we can  move it to the idle device this is very useful  
[3:23:05] actually if you're you have a very limited GPU  and you want to offload the models after using  
[3:23:11] them you can offload them back to the CPU  by moving them to the CPU again and then we  
[3:23:19] load the sampler for now we didn't Define the  sampler but we we use it and later on we build  
[3:23:26] it because it's better to build it after you know  how it is used if we built it before I think it's  
[3:23:33] easy to get lost and what is it happening what's  happening actually so if the sampler name is ddpm
[3:23:44] ddpm then we build the sampler ddpm  sampler we pass it to the noise generator  
[3:23:52] and we tell the sampler how many steps  we want to do for the inferencing
[3:23:58] and I will show you later what why if the  sampler is not edpm then we raise an error  
[3:24:05] because we didn't Implement any other any  other sampler so no sampler let me do f
[3:24:16] sampler name okay why we need to tell him how  many steps because as you remember let's go here
[3:24:26] here this scheduler needs to do many steps how  many we tell him exactly how many we want to  
[3:24:32] do so in this case we do to the the notification  steps will be 50. we even if during the training  
[3:24:39] we have maximum of 1000 steps during inferencing  we don't need to do 1000 steps we can do less of  
[3:24:45] course usually the more steps you do the better  the quality because the more the the the more  
[3:24:50] noise you can remove uh but with the different  Samplers they work in different way and with ddpm  
[3:24:57] usually 50 is good enough to get a nice result  for some other sampler for example ddim you can  
[3:25:02] do less steps for some other Samplers that work on  with differential equations you can do even less  
[3:25:08] depends on which sampler you use and how how lucky  you are with the particular prompt actually also
[3:25:22] this is the latents that will run through the unit  and as you know it's of size latents are hate and  
[3:25:31] latents width which we defined before so it's a  512 divided by 8 by 512 divided by 8 so 64 by 64.  
[3:25:41] and now let's do what what happens if the user  specifies an input image so if we have a prompt  
[3:25:49] we can take care of the prompt by either running a  classifier free guidance which means combining uh  
[3:25:58] um you know the output of the model with the  prompt and without the prompt according to this  
[3:26:05] scale here or we can directly just ask the model  to Output only one image only using the prompt  
[3:26:13] but then we cannot combine the two output with  this scale what happens however if we don't want  
[3:26:20] to do text to image but we want to do image to  image if we do image to image as we saw before  
[3:26:25] we start with an image we encode it with the  encoder and then we add the noise to it and  
[3:26:30] then we ask the scheduler to remove noise noise  noise but since the unit will also be conditioned  
[3:26:37] by the text prompt we hope that while the unit  will denoise this image it will move towards it  
[3:26:45] will move it towards this prompt so this is what  we will do first of things we load the image and  
[3:26:50] we encode it and we add the noise to it so if  an input image is specified we load the encoder
[3:27:05] we move it to the device in case we are using Cuda  for example then we load the tensor of the image
[3:27:17] we resize it we make sure that it's 512 by 512
[3:27:26] with eight and then we transform it into  a numpy array and then into a tensor
[3:27:54] so what will be the size here it will be height  by width by Channel and the channel will be 3.  
[3:28:03] the next thing we do is we rescale this image what  does it mean that they are the input of this unit  
[3:28:09] should be normalized between should be sorry  rescaled between -1 and plus one because if we  
[3:28:16] load the image it will have three channels each  channel will be between 0 and 255 so each pixel  
[3:28:24] have three channels RGB and each number is between  0 and 255 but this is not what the unit wants as  
[3:28:30] input the unit wants every channel every pixel  to be between -1 and plus one so we will do this
[3:28:41] we will build it later this  function it's called the rescale
[3:28:47] to transform anything from  that is from between 0 and  
[3:28:51] 255 into something that is between -1 and plus one
[3:28:57] and this is the will not change the size of  the um of the tensor we add the batch dimension
[3:29:07] on squeeze this adds the batch dimension
[3:29:16] batch size
[3:29:19] okay and then we change  the order of the dimensions
[3:29:32] which is 0 and 3 1 2. why because as you know  the encoder of the variational autoencoder wants  
[3:29:42] batch size Channel height and width while  we have pet size height width channel so we  
[3:29:49] permute them so to obtain the correct input  for the encoder uh this one go into Channel
[3:30:03] and height and width and then this part we  can delete okay this is the input then what  
[3:30:11] we do is we sample some noise because as you  remember the encoder to run the encoder we  
[3:30:17] need some noise and then he will sample from this  particular gaussian that we have defined before  
[3:30:24] so encoder Noise We sample it from our generator
[3:30:31] so as you we have defined this  generator so that we can Define  
[3:30:35] only one seed and we can also make the output  deterministic if we never change the seed  
[3:30:42] uh and this is why we use  the generator latent shape
[3:30:54] okay and now let's run it through  the decoder around the image through
[3:31:02] the of the VA this will produce latents
[3:31:10] for input image tensor and then we give it some  noise now we are exactly here we produced this  
[3:31:19] this is our latency so we give the image to  the encoder along with some noise it will  
[3:31:24] produce a latent representation of this image  now we need to tell our um as you can see here  
[3:31:32] we need to add some noise to this latent  how can we add noise we use our scheduler  
[3:31:37] the strength basically tells us the strength  parameter that we defined here tells us how much  
[3:31:45] we want the model to pay attention to the input  image when generating the output image the more  
[3:31:52] the strength the more the noise we add so the  the more the strength the more the strong the  
[3:31:59] noise so the model will be more creative because  the model will have more noise to remove and can  
[3:32:06] create a different image but if we add less noise  to this initial image the model cannot be very  
[3:32:12] creative because the most of the image is already  defined so there is not much noise to remove so  
[3:32:18] we expect that the output will resemble more or  less the input so this strength here basically  
[3:32:25] means the more noise you how much noise to add  the more noise we add the less the output will  
[3:32:32] resemble the input the less noise we add the more  the output will resemble the input because the the  
[3:32:38] scheduler the the unit sorry has less possibility  of changing the image because there is less noise
[3:32:48] so let's do it first we tell the sampler what  is the strength that we have defined and later  
[3:32:54] we will see what is this method doing but  for now we just write it and then we ask  
[3:33:00] the sampler to add the noise to our latents here  according to the strength that we have defined
[3:33:11] and Noise
[3:33:22] basically the the sampler will CR by setting  the strength will create a Time step schedule  
[3:33:29] later we will see it and by defining these time  steps schedule it will we will start what is the  
[3:33:36] initial noise level we will start with because  if we set the noise level to be for example the  
[3:33:40] strength to be one we will start with the maximum  noise level but if we set the strength to be 0.5  
[3:33:46] we will start with half noise not all completely  noise and later this will be more clear when we  
[3:33:54] actually visit the sampler so now just remember  that we are exactly here so we have the image we  
[3:33:59] transform the compress it with the encoder became  a latent we added some noise to it according to  
[3:34:05] the strength level and then we need to pass it  to the model to the diffusion model so now we  
[3:34:11] don't need the encoder anymore we can set it to  the idle device if the user didn't specify any  
[3:34:19] image then how can we start the denoising  it means that we want to do text to image  
[3:34:24] so we start with random noise so we start with  random noise let's sample some randomize then
[3:34:38] generator and devices device so let  me write some comments if we are doing  
[3:34:47] the text to image start with random noise  randomize defined as n01 for n0i actually
[3:35:01] um we then finally load the  diffusion model which is our unit  
[3:35:08] diffusion its models diffusion later we  see what is this model then how to load it  
[3:35:15] will take it to our device where we are working  so for example Cuda and then our sampler will  
[3:35:23] Define some time steps time steps basically  means that as you remember to train the model  
[3:35:29] we have maximum of 1000 time steps but when we  inference we don't need to do 1001 steps we in  
[3:35:35] our case we will be doing for example 50 steps of  inferencing if the maximum strength level is 1000  
[3:35:43] for example if the maximum level is 1000 the  minimum level will be 1 or if the maximum level is  
[3:35:50] 999 the minimum will be zero and this is a linear  time steps if we do only 50 it means that we need  
[3:35:57] to do for example we start within 1000 and then  we do every 20 so 980 then 960 940 920 900 10 800  
[3:36:10] um what 880 860 840 820 etc etc  until we arrive to the zeroth level  
[3:36:20] basically each of these time steps indicates  a noise level so we with the noise when we  
[3:36:27] denoise the the image or the initial noise  in case we are doing the text to image we  
[3:36:33] can tell the scheduler to remove noise  according to particular time steps which  
[3:36:39] are defined by how many inference steps we want  and this is exactly what we are going to do now  
[3:36:45] when we initialize the sampler we tell him how  many steps we want to do and He will create these  
[3:36:51] times time step schedule so according to how  many we want and now we just go through it so  
[3:36:57] we test the time steps we create tqdm which  is a progress bar we take the time steps
[3:37:06] and for each of these time  steps we denoise the image
[3:37:15] so we have 1 300 this is our we need to tell the  unit as you remember diffusion the unit has as  
[3:37:24] input the time embedding so what is the time step  we want to denoise the context which is the prompt  
[3:37:31] or in case we are doing a classifier free guidance  also the unconditional prompt and the latent the  
[3:37:38] current state of the latent because we will  start with some latent and then keep denoising  
[3:37:43] it and keep denizing it you keep denizing it  according to the time embedding to the time step  
[3:37:49] so we calculate first that I'm embedding  which is an embedding of the current timestamp  
[3:37:55] and we will obtain it from this function
[3:38:01] later we Define it this function basically  will convert a number so the time step into  
[3:38:05] a vector one of size what 320 that  describes this particular time step  
[3:38:13] and as you will see later it's basically  just equal to the positional encoding  
[3:38:18] that we did for the Transformer model so in  the Transformer model we use the signs and  
[3:38:22] cosines to define the position here we use  the signs and codon to define the time step  
[3:38:28] and let's build the model input which is the  latency which is of shape at size 4 because it's  
[3:38:37] the the input of the encoder of the variational  encoder which is of size 4. sorry which has  
[3:38:46] four channels and then has latents height and  the latents with width which is a 64 by 64.
[3:38:58] now if we do if we do this one we need to send  basically we are sending the conditioned what  
[3:39:08] is it here we send the conditional input but also  the unconditional input if we do the classifier  
[3:39:15] free guidance which means that we need to send the  same latent with the prompt and without the prompt  
[3:39:21] and so what we can do is we can repeat this latent  twice if we are doing the classifier for guidance
[3:39:30] it will become a model input by repeat
[3:39:35] on one this will basically  transform batch size four
[3:39:44] Leones
[3:39:50] so this is um I'm going to be twice the size  of the initial batch size which is one actually  
[3:40:00] and the four channels and latent's height and  latent switch so basically we are repeating  
[3:40:06] this Dimension twice we are making two copies  of the latents one will be used with the prompt  
[3:40:12] one without the prompt so now we do we need to  check the model output what is the model output  
[3:40:19] is this the predicted Noise by the unit so the  model output is the predicted Noise by the unit
[3:40:31] we do diffusion model input
[3:40:38] context and time embedding and if we do a  classifier free guidance we need to combine  
[3:40:47] the conditional output and the unconditional  output because we are passing the input of  
[3:40:54] the model if we are doing classified free  guidance we are giving a batch size of  
[3:40:57] two the model will produce an output that has  batch size of two so we can then split it into  
[3:41:04] two different tensor one will be the conditional  and one will be the unconditional so the output  
[3:41:11] conditional and the output unconditional  are splitted in this way using chunk
[3:41:19] the dimension is the along the zeroth dimension  so by default it's the zeroth dimension  
[3:41:26] and then we combine them according  to this formula here what is the  
[3:41:33] I miss out according to this formula  here so unconditional output minus the  
[3:41:41] sorry the condition at output minus  the unconditional output multiplied  
[3:41:45] by the scale that we defined plus the  unconditioned output so the model output  
[3:41:53] will be condition at scale multiplied by  the output conditioned minus the output  
[3:42:01] unconditioned plus the output unconditioned  and then what we do is basically okay now  
[3:42:11] comes the let's say the clue part so we have a  model that is able to predict the noise in the  
[3:42:19] current latency so we start for example imagine  we are doing text to image so let me go back here
[3:42:28] you are going um text to image is a  
[3:42:34] here so we start with some random noise and  we transform into latency then According to  
[3:42:42] some scheduler According to some time  step we keep denoising it now our unit  
[3:42:48] will predict the noise in the latency but how can  we remove this noise from the image to obtain a  
[3:42:58] less noisy image this is done by the scheduler so  at each step we ask the unit how much noise is in  
[3:43:06] the image we remove it and then we give it again  to the unit and ask how much noise is there and  
[3:43:12] we remove it and then I ask again how much noise  is there and then we remove it and then how much  
[3:43:16] noise is there and when will until we finish  all these time steps after we have finished  
[3:43:21] this time steps we take the latent give it to  the decoder which will build our image and this  
[3:43:28] is exactly what we are doing here so imagine we  don't have an input image so we have some random  
[3:43:33] noise we Define sometime steps on this sampler  based on how many inference steps we want to do  
[3:43:41] we do all this time step we give the latents  to the unit the unit will tell us how much  
[3:43:48] is the predicted noise but then we need  to remove this noise so let's do it so  
[3:43:52] let's remove this noise so the latents are  equal to sampler dot step time step latents  
[3:44:01] model output this basically means take  the image from a more noisy version  
[3:44:12] okay let me write it better remove noise  predicted by the unit okay and this is  
[3:44:23] our Loop of the noising then we can do two idle  diffusion now we have our denoised image because  
[3:44:34] we have done it for many steps now what we do  is we load the decoder which is models decoder
[3:44:47] and then our images is run our image is run  through the decoder so we run the latents  
[3:44:57] through the decoder so we do this step here  so we run this latent through the decoder this  
[3:45:01] will give the image it actually will be only  one image because we only specify one image
[3:45:10] then we do images is equal to because the  image was initially as you remember here it  
[3:45:17] was rescaled so from 0 to 255 in in a new scale  that is between 0 minus one and plus one now we  
[3:45:26] do the opposite step so rescale again from -1  to 1 into 0 to 255 with clamp equal through  
[3:45:39] little we will see this function it's  very easy it's just a rescaling function  
[3:45:44] we permute because um to save the image on the CPU  we want the channel Dimension to be the last one  
[3:45:51] permute zero two three one so this  one basically will take the batch size
[3:46:01] a channel 8 width into
[3:46:08] batch size oops tight with Channel
[3:46:18] and then we move the image to the CPU
[3:46:31] and then we convert it into a numpy  array and then we return the image  
[3:46:37] voila let's build this rescale method
[3:46:45] so what is the old scale old range  what is the new range and the clamp
[3:46:55] so let's define the old minimum old maximum  is the old range new minimum and new maximum
[3:47:07] new range
[3:47:10] minus equal to Old mean multiply equal  to new Max minus new mean divided by  
[3:47:25] all the max minus all mean  X Plus equal to new mean  
[3:47:32] we are just rescaling so convert something  that is within this range into this range  
[3:47:39] and if it's clamped then X is equal to x dot  clamp new mean new marks and then we'll turn X
[3:47:54] then you have the time embeddings the the  method that we didn't Define here this get  
[3:47:59] time embedding this means basically take  the time step which is a number so which  
[3:48:03] is a an integer and convert it into a vector  of size 320 and this is will be done exactly  
[3:48:10] using the same system that we use for the  transformer for the positional embeddings
[3:48:18] so we first Define the frequencies of  our cosines and the signs exactly using  
[3:48:24] the same formula of the Transformer so  if you remember the formula is equal to  
[3:48:29] the ten thousand one over ten thousand to the  power of something of I I remember correctly
[3:48:37] so it's power of ten thousand  and minus torch dot arrange so  
[3:48:45] I am referring to this formula just in case  you forgot let me find it using the slides
[3:48:54] I am talking about this formula here so the  formula that defines the positional encodings here
[3:49:09] here we just use a different  dimension dimension of the embedding
[3:49:23] this one will produce  something that is a 160 numbers
[3:49:29] and then we multiply it we we created the  
[3:49:34] we multiplied with the time step  so we create a shape of size one
[3:49:41] so X is equal to torch dot tensor  which is a single time step of T type
[3:49:55] take everything we have the  one dimension so we add one  
[3:50:01] dimension here this is like a doing  unsqueezed multiplied by the frequency
[3:50:11] and then we multiply this by the signs in  
[3:50:13] the cosine just like we did  in the original Transformer
[3:50:21] this one will return a tensor of  size 100 by 62 so which is 320.  
[3:50:29] because we are concatenating to tensors
[3:50:40] not cosine but sine of x and then I concatenated  along the dimension the last dimension  
[3:50:47] and this is our time embedding so now  let's review what we have built here  
[3:50:53] we built basically a system a method that takes  the prompt the unconditional prompt also called  
[3:51:00] the negative prompt The Prompt or empty string  because if we don't want to use any negative  
[3:51:07] prompt the input image so what is the image we  want to start from in case we want to do an image  
[3:51:12] to image the strength is how much attention we  want to pay to this input image when we denoise  
[3:51:18] the image or how much noise we want to add it  to it basically and the more noise we add the  
[3:51:25] less the output will resemble the input image  the if we want to do classifier free guidance  
[3:51:31] which means that if we want the model to Output 2  output one is the output with the prompt and one  
[3:51:37] without the prompt and then we can adjust how much  we want to pay attention to the prompt according  
[3:51:43] to this scale and then we defined the scheduler  which is only one ddpm and we will Define it now  
[3:51:50] and how many steps we want to do the first thing  we do is we create a generator which is just a  
[3:51:56] random number generator then the second thing we  do is if we want to do classifier free guidance  
[3:52:01] as we need to do the basically we need to go  through the units twice one with the prompt one  
[3:52:06] without the prompt the the thing we do is that  actually we create a batch size of two one with  
[3:52:11] the prompt and one without the prompt or using  the unconditional prompt or the negative prompt  
[3:52:18] in case we don't do the classifier free guidance  we only build one tensor that only includes the  
[3:52:23] prompt the second thing we do is we load if  there is an input image we load it so instead  
[3:52:30] of starting from random noise we start from  an image which is to which we add the noise  
[3:52:34] according to the strength we have defined then  for the number of steps defined by the sampler  
[3:52:40] which are actually defined by the number of  inference steps we have defined here we do a  
[3:52:45] loop for Loop that for each for Loop the uh let  me go here the unit will predict some noise and  
[3:52:53] the scheduler will remove this noise and give  a new latent then this new latent is fed again  
[3:52:59] to the unit which will predict some noise and we  remove this noise according to the scheduler then  
[3:53:04] we again predict some noise and we remove the  Some Noise the only thing we need to understand  
[3:53:09] is how we remove the noise from the image now  because we know that the unit is trained to  
[3:53:14] predict the noise but how do we actually remove  it and this is the job of the scheduler so now  
[3:53:20] we need to go build this scheduler here so  let's go build it let's start building our  
[3:53:27] um tdpm scheduler so ddpm.py oops I forgot to put  it inside the folder and let me review one yeah
[3:53:42] this is wrong okay sorry import storage import  Moon pi and let's close the class tdpm sampler  
[3:53:54] okay I didn't call it scheduler because I  don't want you to be confused with the beta  
[3:54:00] schedule which we will Define later so I call  it scheduler here Oops why I opened this one  
[3:54:08] I call it scheduler here but actually I I  mean the sampler because there are there  
[3:54:13] is the beta schedule that we will Define now  what is the beta schedule which indicates the  
[3:54:17] amount of noise at each time step and then  there is what is known as the scheduler or  
[3:54:22] the sampler from now on I will refer it to  as sampler so this scheduler here actually  
[3:54:27] means a sampler I'm sorry for the confusion I  will update the slides when the video is out
[3:54:43] so how much were the training steps  
[3:54:47] which is one thousand the beta is um okay now  I Define two constant and later I did I Define  
[3:54:55] them where what are they and where they come from  0 85 and beta and it's a floating point of 0.0120
[3:55:08] okay the parameter Beta start and beta  end basically if you go to the paper  
[3:55:14] if we look at the forward process we can see that  the forward process is the the process that makes  
[3:55:21] the image more noisy we add noise to the image  so given an image that don't have a have less  
[3:55:28] noise how to get a more noisy image according  to this gaussian distribution which is actually  
[3:55:34] a chain of gaussian distribution which is called  a Markov chain of gaussian distributions and the  
[3:55:41] noise that we add varies according to a schedule  variance schedule beta 1 beta 2 Beta 3 beta 4 beta  
[3:55:49] T So beta basically it's a series of numbers that  indicates the variance of the noise that we add  
[3:55:56] with each of these steps and as in the latent  in the stable diffusion they use a Beta start  
[3:56:05] so the first value of beta is 0.0085 and the  last variance so this the beta that will turn  
[3:56:13] the image into complete noise is equal to 0.010 D  it's a choice made by the authors and and the it's  
[3:56:22] a linear we will use a linear schedule actually  there are other schedules which are for example  
[3:56:27] the cosine schedule Etc but we will be using  the linear one and um we need to Define this  
[3:56:35] better schedule which is actually 1000 numbers  between Beta start and Beta And so let's do it
[3:56:46] so this is defined using the linear space where  the starting number is a better start actually  
[3:56:51] to the power to the square root of Beta start  so square root of beta starts because this is  
[3:56:58] how they Define it in the stable diffusion  if you check the official repository they  
[3:57:04] will also have these numbers and defined in  exactly the same way 0.5 then the number of  
[3:57:11] training steps so in how many pieces we  want to divide this linear space beta and
[3:57:21] and then the the type is torch dot float32 I  think and then to the power of 2 because they do  
[3:57:30] um they divide it into 1000 and then  to the power of 2. and this is in the  
[3:57:36] diffusers libraries from hugging phase I think  this is called the scaled linear schedule  
[3:57:42] now we need to Define an other constant that are  needed for our forward and our backward process so  
[3:57:48] our forward process depends on this beta schedule  but actually this is only for the single step so  
[3:57:53] if you want to go from for example the original  image by one step forward of more noise we need  
[3:57:59] to apply this formula here but there is a closed  formula here called this one here that allows you  
[3:58:06] to go from the original image to any noisified  version of the image at any time step between  
[3:58:12] 0 and 1000 using this one here which depends on  Alpha bar that you can see here so the square root  
[3:58:19] of this Alpha bar and the variance also depends  on this Alpha bar what is Alpha bar Alpha bar is  
[3:58:25] the product of alpha going from 1 up to T so if we  are for example we want to go from the time step  
[3:58:33] 0 which is the image without any noise to the  time step 10 which is the image with some noise  
[3:58:39] and remember that time step 1000 means that it's  only noise so we're going to go to timestart 10  
[3:58:46] which means that we need to calculate this as of  1 as1 as2 S3 S2 and up until a S10 and we multiply  
[3:58:56] them together this is the productory and this  a what is this Alpha this Alpha actually is 1  
[3:59:02] minus beta so let's calculate this Alphas  first so Alpha is actually 1 minus beta
[3:59:09] beta self.betas so it becomes floating and  then we need to calculate the product of  
[3:59:19] this Alphas from 1 to T and this is easily done  with the pi torture we pre-compute them basically  
[3:59:28] this is also come prod self-taught Alphas  
[3:59:35] this will create basically the an array where  the first element is the first Alpha so Alpha  
[3:59:41] for example zero the second element is Alpha 0  multiplied by Alpha One the third element is Alpha  
[3:59:49] zero multiplied by alpha 1 multiplied by Alpha  2 Etc so it's a cumulative product let's we say  
[3:59:58] then we create one tensor that represents  the number one and later we will use it
[4:00:05] and so 1.0 okay we save the generator
[4:00:12] save the number of training steps
[4:00:19] and then we create the time step schedule  the time step basically because we want to  
[4:00:25] reverse the noise we want to remove noise we  will start from the more noisy to less noise  
[4:00:31] so we will go from 1000 to 0. initially so  let's say time steps is equal to torch from
[4:00:51] we reverse this so this is from zero to one  thousand but actually we want one thousand to zero
[4:01:00] and this is our initial schedule in case we  want to do 1000 step but later because here  
[4:01:05] we actually specify how many inference  steps we want to do we will change  
[4:01:11] these time steps here so if the user later  specifies less than 1000 we will change it  
[4:01:17] so let's do let's create the  method that will change this  
[4:01:22] time steps based on how many actual steps we  want to make so set in France step time steps
[4:01:37] as I said before we usually perform 50 which is  
[4:01:40] also actually the one they use normally  for for example in hugging face Library
[4:01:49] let's save this value because we read it later  now if we have a number for example we go from  
[4:01:56] 1000 actually it's not from this is not from zero  to one thousand but it's from zero to one thousand  
[4:02:02] minus one because this is excluded so it will  be from 99 999 998 997 996 Etc up to zero so we  
[4:02:14] have one thousand numbers but we don't want 1000  numbers we want less we want 50 of them so what  
[4:02:20] we do is basically we space them every 20 so we  start with 999 then 999 minus 20 then 999 minus 40  
[4:02:30] etc etc until we arrived to zero but in total here  will be 1000 steps and here will be 1000 50 steps  
[4:02:40] why minus 20 because 20 is 1000  divided by 50 if I'm not mistaken
[4:02:48] so this is exactly what we are going  to do so we can make this step ratio  
[4:02:52] which is self taught training step  divided by how many we actually want
[4:03:04] and we redefine the time steps according  to how many we actually want to make
[4:03:16] zero no inference steps  multiplied by this step ratio
[4:03:24] and round it
[4:03:30] will reverse it just like before because  this is from zero so this is actually  
[4:03:35] means 0 then 20 then 40 then 60 Etc  until we reach 999 then we reverse it
[4:03:44] then copy as type print MP Dot and 64 so  a long one and then we Define as tensor
[4:04:02] uh now the code looks very different from each  other because actually I have been copying the  
[4:04:07] code from multiple sources maybe one of them I  think I copied from the hugging face Library so  
[4:04:13] I didn't change it I I kept it to the original  one okay but the idea is the one I showed you  
[4:04:19] before so now we set the exact number of time  steps we want and we redefine these time steps  
[4:04:25] array like this let's define the next method  which basically tells us let's define the method  
[4:04:34] on how to add noise to something so imagine we  have the image as you remember to do image to  
[4:04:40] image we need to add noise to this latent how do  we add noise to something well we need to apply  
[4:04:46] the formula as defined in the paper so let's go in  the paper here we need to apply this formula here  
[4:04:54] and that's it this is it means that given  this image you need I want to go to the noise  
[4:05:00] noisified version of this image at time step t  which means that I need to take um we need to  
[4:05:09] have a sample from this gaussian but we don't  okay let's build it and we will apply the same  
[4:05:15] trick that we did for the variational autoencoder  as you remember in the variational auto encoder  
[4:05:20] I actually already showed how we sample from  a distribution of which we know the mean and  
[4:05:24] the variance here we will do the same here but  we of course we need to build the mean and the  
[4:05:29] variance what is the mean of this distribution  it's this one and what is the variance it's this  
[4:05:35] one so we need to build the mean and the variance  and then we sample from this so let's do it ddpm
[4:05:50] so we take the original samples
[4:05:54] which is a flow tensor and then the time steps
[4:06:01] so this is actually time step not time steps  it indicates at what time step we want to add  
[4:06:06] the noise because you can add the time Step at  the denoise at time step one two three four up  
[4:06:11] to one thousand and with each level the noise  increases so the noisified version at the time  
[4:06:17] step one will be not so noisy but at one at  the time step 1000 will be complete noise
[4:06:25] this returns a float tensor
[4:06:30] okay we get let's calculate first let me check  what we need to calculate first we can calculate  
[4:06:39] first the mean and then the variance so to  calculate the mean we need this Alpha Chrome  
[4:06:45] product so the cumulative process of the alpha  which stands for Alpha bar so the alpha bar as  
[4:06:51] you can see is the cumulative product of all  the alphas which is each Alpha is 1 minus beta  
[4:06:56] so we take this Alpha bar which we will call  Alpha complete so it's already defined here  
[4:07:04] Alpha is self-taught 1.2 device
[4:07:12] we move it to the same device because  we need to later combine it with it
[4:07:20] and of the same type
[4:07:26] this is a tensor
[4:07:30] that we also move to the same device of  the other tensor now we need to calculate  
[4:07:36] the square root of alpha bar so let's  do it square root of alpha [ __ ] prod
[4:07:49] at the time step t to the power of 0.5 y to the  power of 0.5 because having a number to the power  
[4:08:00] of 0.5 means doing its the the square root of  the number because the square root of one half  
[4:08:06] which becomes the square sorry to the power of  one half which becomes the square root and then  
[4:08:13] we flatten this array
[4:08:17] and then basically because we need to  combine this Alpha comprot which doesn't  
[4:08:22] have Dimensions it only has one dimension  which is the number itself but we need to  
[4:08:26] combine it with the latents we need to  add some Dimensions so one trick is to  
[4:08:30] just keep adding Dimensions with unsqueeze  until you have the same number of dimensions
[4:08:39] shape is less than
[4:08:46] most of this code I have taken from  the hugging phase libraries samplers
[4:09:01] so we keep our animation until this one and  this tensor and this sensor have the same  
[4:09:06] dimensions this is because otherwise we cannot  do broadcasting when we multiply them together  
[4:09:11] the other thing that we need to calculate this  formula is this part here 1 minus Alpha bar so  
[4:09:18] let's do it so s Square t of 1 minus Alpha  prod as the name implies is 1 minus Alpha  
[4:09:28] comproed at the time step t to the power of 0.5  y 0.5 because we don't want the variance we want  
[4:09:38] the standard deviation just like we did with the  encoder of the variational auto encoder we want  
[4:09:44] the standard deviation because as you remember  if you have an N 0 1 and you want to transform  
[4:09:50] into an N with a given mean and the variance the  formula is X is equal to mean plus the standard  
[4:09:56] deviation multiplied by the n01 let's go back  so this is the standard deviation maybe shown
[4:10:09] and we also flatten this one
[4:10:15] and then again we keep adding the dimensions  until they have the same dimension
[4:10:23] otherwise you cannot multiply them  together or sand them together
[4:10:32] and squeeze so we keep adding Dimensions  now as you remember we our method should  
[4:10:42] add the noise to an image so we need to add  noise means we need to sample some noise  
[4:10:48] so we need to sample some noise from the n01
[4:11:00] using this generator that we have
[4:11:10] I think my cat is very angry today with  me because I didn't play with him enough  
[4:11:15] so later if you guys excuse me I need to later  play with him I think we will be dying we will  
[4:11:22] be done very soon so so let's get the noisy  samples using the noise and the mean and  
[4:11:28] the variance that we have calculated according  exactly to this formula here so we do the mean
[4:11:37] actually no the mean is this  one multiplied by X zero  
[4:11:42] so the mean is this one multiplied by x 0  is the mean so we need to take this square  
[4:11:47] root of alpha comprot multiplied by X zero  and this will be the mean so the mean is  
[4:11:53] the square root of alpha product multiplied by  the original latent so x 0 so the input image or  
[4:11:58] whatever we want to noise if I plus the standard  deviation which is the square root of this one  
[4:12:06] multiplied by a sample of the from the n01 so  the noise and this is how we notify an image
[4:12:17] this is how we add the noise to an image  so this one let me write it down so all  
[4:12:22] of this is according according to the equation  4 of the DDM paper and also according to this
[4:12:39] okay now that we know how to add noise  
[4:12:44] we need to understand how to remove noise  so as you remember let's review again  
[4:12:49] here imagine we are doing the text to text or  email text to image or image to image it doesn't  
[4:12:56] matter the point is other unit as you remember  is trained to only predict the amount of noise  
[4:13:02] given the latent with noise given the prompt  and the time Step At which this noise was added  
[4:13:11] um so what we do is we have this predicted noise  from the unit we need to remove this noise so  
[4:13:20] the unit will predict the noise but we need some  way of removing the noise to get the next latent  
[4:13:25] what I mean by this is um you can see this uh  reverse process here so the reverse process is  
[4:13:34] defined here we want to go from x t so something  more noisy to something less noisy based on the  
[4:13:44] moon or anyway based on the noise not on the moon  sorry based on the noise that was predicted by the  
[4:13:50] unit but here in this formula you don't see any  relationship to the noise predicted by the unit  
[4:13:57] actually here it just says if you have a met  if you have a network that can evaluate this  
[4:14:05] mean and this variance you know how to remove  the noise to how to go from XT to XT minus one  
[4:14:11] but we don't have a method that actually predicts  the mean and the variance we have a method that  
[4:14:16] tells us how much noise is there so the formula  we should be looking at is actually here so here  
[4:14:25] here because we have um we trained our  networkers our unit as a Epsilon Theta as  
[4:14:33] you remember our Training Method was this we  do gradient descent on this loss in which we  
[4:14:40] train a network to predict the noise in a noisy  image so we need to use this Epsilon Theta now  
[4:14:48] to remove the noise so this predicted noise  to remove the noise and if we read the paper  
[4:14:53] it's written here that to sample x t minus 1  given x t is to compute x t minus 1 is equal to  
[4:15:03] this formula here this tells us how to go  from x t to x t minus 1. and this is the  
[4:15:11] so basically we sample Some Noise We multiplied  by this Sigma and this basically reminds us on  
[4:15:19] how to move go from the n01 to any distribution  with a particular mean and a particular variance  
[4:15:26] so we will be working according to this formula  here actually because we have a model that  
[4:15:31] predicts noise here this Epsilon Theta and this  is our unit the unit is trained to predict noise  
[4:15:39] so let's build this part now and I will while  building it I will also tell you which formula I'm  
[4:15:44] referring to at each step so you can also follow  the paper so now let's build the method let's call  
[4:15:49] step method that given the time Step at which the  noise was added or we think it was added because  
[4:15:57] when we do the reverse process we can also Skip  it's not we think it was other but we can skip  
[4:16:04] some time steps so we need to tell him what is  the time Step at width it should remove the noise  
[4:16:10] the latents so as you know the unit works with  the latent so with this disease here so this is z  
[4:16:17] and it keeps the noising so the latents and then  what is the model output so the predicted all  
[4:16:25] noise of the unit so the the model output is  the predicted noise torch dot then sort this  
[4:16:36] model output corresponds to this Epsilon Theta of  X DT so this is the predicted noise at time step t  
[4:16:46] this latent is our XT and what else we need the  alpha we have the beta we have we have everything  
[4:16:55] okay let's go so T is equal to time step  the previous T is equal to self dot get  
[4:17:05] previous time step this is a function that  given this time step calculates the previous  
[4:17:12] one later we will build it actually we  can build it now it's very simple um
[4:17:23] okay
[4:17:31] get previous time step the self I start  which is an integer we return another integer  
[4:17:41] previous time step is equal to the time step  
[4:17:44] minus self minus basically this quantity here  step ratio so self dot num training steps
[4:17:56] divided by itself dot num inference steps  return previously this one will return basically  
[4:18:07] um given for example the number 999 it will  return number 999 minus 20. because the the  
[4:18:16] time steps for example the initial time step will  be suppose it's 1000 the training steps we are  
[4:18:23] doing is 1000 divided by the number of inference  step which is we will be doing is 50. so this is  
[4:18:28] means 1000 minus 20 because a thousand y divided  by 50 is 20. so it will return 980. when we give  
[4:18:36] him a 980 as input he will return 960. so what is  the next step that we will be doing uh in our for  
[4:18:43] Loop or what is the previous step of the denoising  so we are going from the image noise at the time  
[4:18:50] step 1000 to an image noise that time step 980  for example this is the meaning of previous term  
[4:18:59] then we retrieve some data later  we will use it so Alpha for the  
[4:19:04] t is equal to self dot Alpha for now if  you don't understand don't worry because  
[4:19:09] later I will write I will just collect  some data that we need to calculate the  
[4:19:12] formula and then I will tell you exactly  which formula we are going to calculate
[4:19:19] um Alpha prod
[4:19:34] if we don't have any previous step then we don't  know which Alpha to return so we just returned one
[4:19:45] and actually there is a paper that came out  I think from by dance that was complaining  
[4:19:51] that this method of doing is not correct because  the the last time step doesn't have this is not  
[4:19:58] doesn't have the signal to noise ratio about  equal to zero but okay this is something we  
[4:20:03] don't need to care about now actually if you're  interested I will link the paper in the comments
[4:20:19] prev current Alpha Team
[4:20:26] divided by Alpha graph current  also this code I took it from  
[4:20:35] hugging face the diffusers Library because I mean  we are applying formula so even if I wrote it by  
[4:20:43] myself it wouldn't be any different because we  are just applying formulas from the paper so  
[4:20:49] the first thing we need to do is to compute  the original sample according to the formula  
[4:20:54] 15 of the paper what do I mean by this as  you can see where is it this one where is it
[4:21:09] here so actually let me show  you another formula here  
[4:21:14] as you can see we can calculate the previous step  so the less noise the the forward process sorry  
[4:21:22] the reverse process we can calculate the less  noisy image giving a more noisy image and the  
[4:21:27] predicted image As Time step 0 according to this  formula here where the mean is defined in this  
[4:21:36] way and the variance is defined in this way but  what is the predicted x0 so given an image given  
[4:21:48] um a noisy image at time step T how can we  predict what is the X zero of course this is  
[4:21:54] the predicted x0 not what will be the X zero  so this predicted x 0 we can also retrieve it  
[4:22:02] using the formula number 15 if I remember  correctly it's here so this x 0 is given as x  
[4:22:10] t minus 1 minus Alpha multiplied by the predicted  noise at time step T divided by the square root  
[4:22:17] of alpha all these quantities we have so actually  there are two ways which are equivalent to each  
[4:22:22] other actually numerically of going from more  noisy to less noisy one way is this one this one  
[4:22:29] here which is the algorithm tool of the sampling  and one is uh this one here so the equation number  
[4:22:36] seven that allows you to go from more noises to  less noisy but the two are numerically equivalent  
[4:22:42] they just in the in the effect they are equivalent  it's just um they have different parameterization  
[4:22:48] so they have different formulas so as a matter  of fact for example here in the code they say to  
[4:22:56] go from x t to x t minus one you need to do this  calculation here but as you can see for example  
[4:23:02] this is this numerator of this multiplied by  this Epsilon Theta is different from the one  
[4:23:11] in the algorithm here but actually they are  the same thing because BT is equal to equal  
[4:23:15] to 1 minus Alpha t as beta Alpha is defined as 1  minus beta as you remember so there are multiple  
[4:23:23] ways of obtaining the same thing so what we will  do is we actually we will apply this formula here  
[4:23:29] in which we need to calculate the mean and we  need to calculate the variance according to this  
[4:23:34] formulas here in which we know Alpha we know beta  we know Alpha bar we know all the other Alphas we  
[4:23:40] know because there are parameters that depend  on beta what we don't know is x0 but x 0 can be  
[4:23:45] calculated as in the formula 15 here so first  we will calculate this x0 predicted X is zero
[4:23:59] first compute the predicted original  sample using formula 15 of the ddpm paper
[4:24:13] or predicted original sample latents minus  while so we do latents minus the square  
[4:24:24] root of 1 minus Alpha T what is the square  root of 1 minus Alpha T is equal to Beta  
[4:24:31] so I have here beta t which is  already 1 minus Alpha t as you can see  
[4:24:38] Alpha bar 1 minus Alpha bar at the time step  T because I already retrieve it from here so  
[4:24:45] 1 minus sorry beta to the power 2 to the power  of one half or the square root of beta so we do  
[4:24:52] latents minus beta prod at density to the power of  0.5 which it means basically square root of beta  
[4:25:02] and then we multiply this by the predicted  noise of the image of the latent at time  
[4:25:09] step T so what is the predicted noise  it's the model output because of our  
[4:25:13] unit predicts the noise model output and  then we need to divide this by let me check
[4:25:23] square root of alpha t which we  have I think here Alpha T here  
[4:25:29] so the square root of alpha t  Alpha prod t to the power of 0.5
[4:25:38] here I have something on this one I  don't need this one I don't need okay  
[4:25:46] because otherwise it's wrong right yeah before  first there is a product between these two terms  
[4:25:51] and then there is the difference here okay this is  how we compute the prediction the X zero now let's  
[4:25:57] go back to the formula number seven uh seven seven  okay now we have this x 0 so we can compute this  
[4:26:05] term and we can compute this term and this we can  compute this term and all the other terms we also  
[4:26:10] can compute so we calculate this mean and this  variance and then we sample from this distribution
[4:26:16] so compute the coefficients for red original  sample and the current sample XT this is the  
[4:26:30] same committee that you can find on the diffusers  Library which basically means we need to compute  
[4:26:35] this one this is this coefficient for the  predicted sample and this is the coefficient  
[4:26:39] for x t this one here so predicted original  sample coefficient which is equal to what  
[4:26:49] Alpha product T minus 1 so the previous Alpha  prod t which is Alpha protein previous which  
[4:26:59] means the alpha probability but at the previous  time step under the square root so to the power  
[4:27:07] of 0.5 multiplied by the current beta T so  the beta at the time step T So current beta t  
[4:27:16] which is we Define it here current beta T  we retrieve it from alpha we could have a
[4:27:24] okay and then we divide it by beta product because  1 minus Alpha bar is actually equal to Beta bar  
[4:27:35] beta product t then we have the this  coefficient here so this one here  
[4:27:43] so this is current sample coefficient is  equal to current Alpha t to the power of 0.5  
[4:27:51] which means the square root of this time this  this thing here so the square root of alpha t  
[4:27:58] and then we multiply it by Beta at the  previous time step because it's 1 minus  
[4:28:02] Alpha at the previous time step corresponds  to Beta as at the previous time steps time  
[4:28:07] step multiplied by Beta prod divided by  beta at the time step T So beta prod t  
[4:28:20] now we can compute the mean so the mean  is the sum of these two terms sample
[4:28:29] so let me write some here compute the  predicted previous sample mean modity
[4:28:42] is equal to predicted original sample  coefficient multiplied by what by x 0  
[4:28:48] what is X 0 is this one that we obtained  by the formula number 15 so the prediction  
[4:28:53] predicted original sample so x0 plus this term  here what is this term is this one here so the  
[4:29:01] current sample coefficient multiplied by x t  what is x t is the latent at the time step t
[4:29:11] now this we have computed the mean for  now we need to compute also the variance  
[4:29:17] let's create another method  to compute the variance
[4:29:22] that get variance 12 time step and
[4:29:32] okay we obtained the previous time test T  because we need to do four letter calculations
[4:29:40] again we calculate the alpha  prod T so all the terms that  
[4:29:44] we need to calculate this particular terms here
[4:30:15] and the current beta T is equal to 1 minus  Alpha probability division Alpha from this one  
[4:30:26] what is current beta T is equal to 1 minus  Alpha protein yeah 1 minus Alpha protein  
[4:30:36] divided by Alpha property zero okay
[4:30:43] so the variance according to the formula  number six and seven so this formula here
[4:30:52] is given as 1 minus Alpha for the D  Prime so 1 minus Alpha probability Prime
[4:31:03] divided by 1 minus Alpha prod which is 1 minus  Alpha prod y prod because this is the alpha bar  
[4:31:13] and multiplied by the current beta  beta t and beta T is defined I don't  
[4:31:22] remember where it's 1 minus Alpha and  this is our barbariance we clamp it
[4:31:32] oops third dot clamp variance and the  minimum that we want is one my equal  
[4:31:42] to minus 20. to make sure that it doesn't  reach 0. and then we return the variance
[4:31:52] and now that we have the mean and the  variance so this variance has also been  
[4:31:57] computed using I mean right here computed  using formula serving of the ddpm paper
[4:32:11] and now we go back to our step motion so what  we do is equal to zero so because we only need  
[4:32:21] to add the variance if we are not at the last last  time step if you are at the last time step we have  
[4:32:27] no noise so we don't add any uh we we don't add  we don't need to add any noise actually because  
[4:32:34] um the point is we are going to sample from  this distribution and just like we did before  
[4:32:39] we actually sample from the n01 and then  we shift it according to the formula so  
[4:32:46] uh the N gaussian with a particular mean and  the particular variance is equal to the uh  
[4:32:55] the gaussian at the zero one multiplied by the  standard deviation plus the um plus the mean so
[4:33:12] you sample the noise
[4:33:34] okay let me sample some  noise you can do the variance
[4:33:41] actually this is the variance already multiplied  by the noises so it's actually the standard  
[4:33:48] deviation because we will see um self Dot get  variance let's see the time step t to the power  
[4:33:57] of 0.5 so this is 0.5 so this one becomes the  standard deviation we multiply it by the n01  
[4:34:06] so what we are doing is  basically we are going from n01  
[4:34:11] to n with a particular move and a particular  Sigma using the usual trick of going from X  
[4:34:18] is equal to the move plus the sigma actually not  yeah this is the sigma Square then because this  
[4:34:26] is the variance Sigma multiplied by the Z where  Z where Z is distributed according to the N 0 1  
[4:34:36] this is the same thing that we always done  also for the variation of the encoder also  
[4:34:40] for adding the noise the same thing that  we did before this is how you sample from  
[4:34:45] a distribution how you actually shift the  parameter of the gaussian distribution
[4:34:52] so predicted prev sample is equal to the predicted  prep sample plus the variance this variance term  
[4:35:00] here already includes the sigma multiplied  by Z and then we return predicted prep sample  
[4:35:09] oh okay now we have also built the the sampler let  me check if we have everything no we missed still  
[4:35:17] something which is the set strength method  as you remember once we want when we want  
[4:35:24] to do image to image so let's go back to check  our slides if we want to do image to image we  
[4:35:30] convert the image using the vae to a latent then  we need to add noise to this latent but how much  
[4:35:36] noise we can decide the more noise we add the  more freedom the unit will have to change this  
[4:35:41] image the last noise we add the less Freedom  it will have to change the image so what we  
[4:35:46] do is basically by setting the strength we make  our sampler start from a particular noise level  
[4:35:53] and this is exactly what the method we want  to implement so I made some s okay so for  
[4:36:00] example as soon as we load the image we  set the strength which will shift the  
[4:36:04] noise level from which we start from and then  we add the noise to our latent to create the  
[4:36:10] image to image here so let's go here and  we create this method called set strength
[4:36:19] strength
[4:36:25] okay the start step because  we will skip some steps  
[4:36:29] is equal to self-taught new inference  steps minus end of cell phone inference
[4:36:42] this basically means that if we have a  50s inference steps and then we set the  
[4:36:47] strength to let's say 0.8 it means that we  will skip 20 of the steps so when we will  
[4:36:53] add we will start from image to image  for example we will not start from a  
[4:36:58] Pure Noise image but we will start from 80  percent of noise in this image so the unit  
[4:37:03] will still have freedom to change this  image but not as much as with 100 noise
[4:37:10] uh we redefined the timestamps  because we are altering the schedule  
[4:37:16] so basically we skip sometimes steps
[4:37:22] and self dot start step is equal to  start step so actually what we do  
[4:37:28] here is suppose we have the strength of 80  we are actually fooling the method the the  
[4:37:33] unit into believing that he came up with  this image which is now with this level  
[4:37:38] of strength and now he needs to keep  denoising it this is how we do image  
[4:37:44] to image so we start with an image we noise  it and then we make the unit believe that he  
[4:37:49] came up with this image with this particular  noise level and now he has to keep the noising it  
[4:37:55] until according of course also to The Prompt  until we reach the clean image without any noise
[4:38:03] now we have the pipeline that we can call we  have the ddpm sampler we have the model built  
[4:38:12] of course we need to create the function to load  the weights of this model so let's create another  
[4:38:18] file we will call it the model loader uh here  model loader because now we are nearly close  
[4:38:27] to sampling from this finally from this stable  diffusion so now we need to create the method  
[4:38:31] to load the pre-trained the pre-trained weights  that we have downloaded before so let's create it
[4:38:40] import clip
[4:38:46] coder or VA encoder then from  decoder import the decoder
[4:38:58] Fusion import Fusion of a  division model which is our unit  
[4:39:04] now let me first Define it then I  tell you what we need to do so Prelude
[4:39:11] from standard weights
[4:39:22] okay as usual we load the  weights using torture but  
[4:39:30] we use we will create another function model  converter dot load from standard weights
[4:39:45] this is a method that we will create later  to to load the weights the pre-trained  
[4:39:50] weights and I will show you why we need  this method then we create our encoder
[4:39:58] and we load the state detector  load status from our set addict
[4:40:07] and we'll set strict to to oops predict
[4:40:21] strict
[4:40:26] true then we have the decoder
[4:40:44] and it's strict also so this strict parameter  here basically tells that when you load a model  
[4:40:51] from pytorch this for example this ckp ckpd file  here it is a dictionary that contains many keys  
[4:40:59] and each key corresponds to one Matrix of  our model so for example this uh self this  
[4:41:06] group normalization has some parameters and  the the hole can torch load this parameters  
[4:41:12] exactly in this group Norm by using the name  of the variables that we have defined here and  
[4:41:18] he will when we load the model from PI torch he  will actually load the dictionary and then we load  
[4:41:25] this dictionary into our models and he will match  by names now the problem is the pre-trained model  
[4:41:30] actually they don't use the same name that I have  used and actually this code is based on another  
[4:41:36] code that I have seen so actually the the names  that we use are not the same as the pre-trained  
[4:41:42] model also because the names in the pre-trained  model not always very friendly for learning this  
[4:41:49] is why I changed the names and also other people  changed the names of the methods but this also  
[4:41:55] means that the automatic mapping between the names  of the pre-train model and the names defined in  
[4:42:02] other classes here cannot happen because it  cannot happen automatically because the names  
[4:42:06] do not match for this reason there is a script  that I have created in my GitHub Library here  
[4:42:14] that you need to download to convert these names  it's just a script that Maps one name into another  
[4:42:20] so if the name is this one map it into this if  the name is this one mapping into this there is  
[4:42:25] nothing special about the script it's just a very  big mapping of the names and this is actually done  
[4:42:31] by most models because if you want to change  the name of the classes and or the variables  
[4:42:37] then you need to do this kind of mapping so I  will also uh I will basically copy it I don't  
[4:42:44] need to download the file so this is called the  model converter dot pi model converter dot pi and  
[4:42:54] that's it it's just a very big mapping of names  and I take it from this comment here on GitHub
[4:43:03] so this is model converter so we need to import  this model converter import model com converter
[4:43:14] this model converter basically will convert  the names and then we can use the load State  
[4:43:18] dictator and this will actually map all the  names now now the names will map with each  
[4:43:23] other and the district makes sure that if there  is even one name that doesn't map then through  
[4:43:27] an exception which is what I want because  I want to make sure that all the names map
[4:43:38] so we Define the diffusion  and we load it's statistic
[4:43:47] diffusion and strict equal to true
[4:43:57] and let me check
[4:44:01] up right
[4:44:07] then we do clip is equal to clip.2 device  so we move it to device where we want to  
[4:44:13] work and then we load also his State  addict so the parameters of the weights
[4:44:30] and then we return a dictionary clip
[4:44:35] clip and then we have the encoder is the encoder  we have the decoder the decoder and then we have  
[4:44:47] the diffusion we have the diffusion Etc now  we have all the ingredients to run finally the  
[4:44:56] inference guys so thank you for being patient  so much and it's really finally we have we can  
[4:45:03] see the light coming so let's build our notebook  so we can visualize the image that we will build  
[4:45:12] okay let's select the kernel stable  diffusion I already created it  
[4:45:18] in my repository you will also find the  requirements that you need to install in  
[4:45:23] order to run this so let's import everything  we need so the model loader the pipeline
[4:45:32] pill import image this is how to load the image  from python so partly import or actually this  
[4:45:41] one we don't need transformers this is the only  library that we will be using because the is the  
[4:45:48] tokenizer of the clip so how to tokenize the  the text into tokens before sending it to the  
[4:45:54] clip embeddings otherwise we need also need to  build the tokenizer and it's really a lot of job
[4:46:10] I don't allow Cuda and I also don't  allow MPS but you can activate these two  
[4:46:17] variables if you want to use good or MPS
[4:46:25] viable and hello Cuda then the  device becomes Cuda of course
[4:47:01] and then we printed the device we are using
[4:47:11] okay let's load the tokenizer tokenizer is  the clip tokenizer we need to tell him what  
[4:47:17] is the vocabulary file so which is already  saved here in the data data vocabulary.json  
[4:47:24] and then also the merges file maybe one day I  will make a video on how the tokenizer works  
[4:47:31] so we can build also the tokenizer but this is  something that requires a lot of time I mean  
[4:47:37] and it's not really related to the diffusion  model so that's why I didn't want to build it  
[4:47:43] the model file is I will use the data and then  this file here then below the model so the models  
[4:47:51] are model loader dot preload model from the model  file into this device that we have selected okay  
[4:48:00] let's build from text to image but we need  to define the prompt for example I want a cat
[4:48:10] sitting or stretching let's  say stretching on the floor  
[4:48:16] highly detailed we need to create  a prompt that will create a good  
[4:48:20] image so we need to add some lot of details  Ultra sharp cinematic etc etc 8K resolution
[4:48:31] the unconditioned prompt
[4:48:35] I keep it blank this you can also use it  as a negative member you can use it as a  
[4:48:42] negative prompter so if you don't want the  some you don't want the more the output to  
[4:48:49] have some how to say some characteristics  you can Define it in the negative prompt  
[4:48:54] of course I like to do CFG so the classifier  freak items which we set true CFG scale is a  
[4:49:04] number between 1 and 14 which indicates how much  attention we want the model to pay to this prompt  
[4:49:09] 14 means pay very much attention or one means  pay very little attention I use seven because  
[4:49:16] then we can Define also the  parameter for image to image
[4:49:23] so input image is equal to none image path  is equal to I will Define it with my image  
[4:49:32] of the dog which I already have here and but  for now I don't want to load it so if you want  
[4:49:40] to load it we need to do input image is equal  to image dot open image part but for now I will
[4:49:51] I will not use it so now let's comment it and  if we use it we need to define the strength so  
[4:49:57] how much noise we want to add to this image but  for now let's not use it the sampler we will be  
[4:50:02] using of course is the only one we have is the  ddpm the number of inference steps is equal to
[4:50:10] 15 and the seed is equal to 42 because it's a  lucky number at least According to some books  
[4:50:21] output image is equal to pipeline generate okay  The Prompt is the prompt that we have defined  
[4:50:31] The unconditioned Prompt is the unconditional  pump that we have defined input image is the  
[4:50:38] input image that we have defined if it's not  commented of course the strength for the image
[4:50:49] and the CFG scale is the one we have defined
[4:50:56] sampler name is the sampler name we have defined  
[4:51:01] the number of inference steps is  the number of inputs the seed models
[4:51:13] device
[4:51:17] idle devices of our CPU so when we don't want to  
[4:51:20] use something we move it to the CPU  and the tokenizer is the tokenizer
[4:51:27] and then image Dot from array output image if  everything is done well if all the code has  
[4:51:38] been written correctly you can always go back to  my repository and download the code if you don't  
[4:51:43] want to write it by yourself let's run the code  and let's see what is the result of my computer  
[4:51:49] will take a while so it will take some time  so let's run it so if we run the code it will  
[4:51:56] generate an image according to our prompt in my  computer it took really a long time so I cut the  
[4:52:01] video and I actually already replaced the code  with the one from my GitHub because now I want  
[4:52:08] to actually explain you the code without while  showing you all the code together how does it  
[4:52:14] work so now we we generated an image using only  the prompt I use the CPU that's why it's very  
[4:52:20] slow because my GPU is not powerful enough and we  set a unconditional prompt to zero we are using  
[4:52:25] the classifier free guidance and with a scale  of seven so let's go in the pipeline and let's  
[4:52:30] see what happens so basically because we are doing  the classifier free guidance we will generate two  
[4:52:37] conditioning signals one with the prompt and one  with empty text which is the unconditioned prompt  
[4:52:45] which is also called the negative prompt this  will result in a batch size of two that will  
[4:52:50] run through the unit so let's go back to here  suppose we are doing text to image so now our  
[4:52:56] unit has two latents that he's doing at the same  time because we have the batch size equal to two  
[4:53:01] and for each of them it is predicting the noise  level but how can we move remove this noise from  
[4:53:10] the predict the predicted noise from the initial  noise so because to generate an image we start  
[4:53:17] from random noise and the prompt initially  we encode it with our vae so it becomes a  
[4:53:23] latent which is still noise and with the unit we  predict we predict how much noise is it according  
[4:53:30] to a schedule so according to 50 steps that of  inferencing that we will be doing at the beginning  
[4:53:36] the first step will be 1000 The Next Step will  be 980 The Next Step will be 960 Etc so this  
[4:53:43] time will change according to this schedule so  that at the 50th step we are to the time step 0.  
[4:53:51] and how can we then with the predicted noise go  to the next latent so we remove this noise that  
[4:54:00] was predicted by the unit well we do it with the  sampler and in particular we do it with the sample  
[4:54:06] method of the sampler uh step method sorry  of the sampler which basically will calculate  
[4:54:12] the previous sample given the current sample  according to the formula number 7 here so which  
[4:54:18] basically calculates the previous sample given  the current one so the less noisy one given the  
[4:54:25] current one and the predicted x0 so this is not x0  because we don't have X zero so we don't have the  
[4:54:31] um the noise the sample without any noise so  but we can predict it given the values of the  
[4:54:38] current noise and the beta schedule another way  of denoising is to do the sampling like this if  
[4:54:45] you watch my other repository about the ddpm  paper I actually implemented it like this if  
[4:54:49] you want to see this version here and this is how  we remove the noise to get a less noisy version so  
[4:54:56] once we get the less noisy version we keep doing  this process until there is no more noises so we  
[4:55:02] are set the time step 0 in which we have no  more noise we give this latent to the decoder  
[4:55:07] which will turn it into an image this is how the  texture image works the image to image on the  
[4:55:12] other side so let's try to do the image to image  so to do the image to image we need to go here  
[4:55:18] and we uncomment this code here this allows us to  start with the dog and then give for example some  
[4:55:27] prompt for example we want a this talk here we  want to say okay we want a dog stretching on the  
[4:55:35] floor highly detailed Etc we can run it I will not  run it because it will take another five minutes  
[4:55:41] and if we do this we can set the strength of  let's say 0.6 which means that let's go here  
[4:55:50] so we set a strength of 0.6 so we have  this input image strength of 0.6 means  
[4:55:55] that we will add we will encode it with the  virational autoencoder will become a latent  
[4:56:00] We'll add some noise but how much noise not  all the noises so that it becomes completely  
[4:56:07] noise but less noise than that so as a let's  let's say 60 noise is not really true because  
[4:56:16] um because it depends on the schedule in our case  it's linear so it can be considered 60 of noise  
[4:56:23] we then give this image to the scheduler which  will start not from the 1000 step it will start  
[4:56:29] before so if we set the strength to 0.6 it will  start from the 600th step and then move by 20 uh  
[4:56:38] will keep going um 600 and 580 then 560 then 100  540 Etc until it reaches 20. so in total it will  
[4:56:47] do less steps because we start from a less noisy  example but at the same time because we start with  
[4:56:54] less noise the the unit also has less freedom  to change the uh to alter the image because he  
[4:57:01] already have the image so he cannot change it too  much so how do you adjust the noise the the noise  
[4:57:08] level depends if you want the unit to pay very  much attention to the input image and do not  
[4:57:15] change it too much then you add less noise if you  want to change completely the original image then  
[4:57:21] you can add all the possible noises so you set the  strength to one and this is how the image to image  
[4:57:26] works I did it implement the impainting because  the reason is that the pre-trained model here so  
[4:57:34] the model that we are using is not fine-tuned  for in painting So if you got on the website  
[4:57:39] and you look at the model card they have another  model for impainting which has different weights  
[4:57:47] here the this one here but this the structure of  this model is also little different because they  
[4:57:54] have in the unit they have five additional input  channels for the mask I will of course implement  
[4:58:02] it in my repository directly so I will modify the  code and also implement the code for impainting so  
[4:58:10] that we can support this model but unfortunately  I don't have the time now because in China here is  
[4:58:15] the goch India and I'm going to lautia with my  my wife so we are a little short of time but I  
[4:58:22] hope that with my video guys you you got really  into stable diffusion and you understood what  
[4:58:27] is happening under the hood instead of just using  the hugging face library and also notice that the  
[4:58:33] model itself is not so particularly sophisticated  if you check the decoder and the encoder they are  
[4:58:41] just a bunch of convolutions and up sampling and  the normalizations just like any other computer  
[4:58:49] vision model and the same goes on for the unit of  course there are very smart choices in how they  
[4:58:54] do it okay but that's not the important thing  of the diffusion and actually if we study the  
[4:59:00] diffusion models like score models you will see  that it doesn't even matter the structure of the  
[4:59:05] model as long as the model is expressive it will  actually learn the score function in the same way  
[4:59:10] but this is not our case in this video I will talk  about score model in future videos what I want  
[4:59:16] you to understand is that how this all mechanism  works together so how can we just learn a model  
[4:59:23] that predicts the noise and then we come up with  images and let me rehearse again the idea so we  
[4:59:31] started by training a model that needs to learn  a probability distribution as you remember P of  
[4:59:38] theta here we we cannot learn this one directly  because we don't know how to marginalize here so  
[4:59:45] what we did is we found some lower Bound for this  quantity here and we maximize this lower bound  
[4:59:50] how do we maximize this lower Bound by training  a model by running the gradient descent on this  
[4:59:56] loss this loss produces a model that allow us to  that predicts the noise then how do we actually  
[5:00:06] use this model with the predicted noise to  go back in time with the noise because the  
[5:00:11] forward process we know how to go it's defined  by us how to add noise but in back in time so  
[5:00:16] how to remove noise we don't know and we do it  according to the formulas that I have described  
[5:00:21] in the sampler so the formula number seven and the  formula number also this one actually we can use  
[5:00:28] actually I will show you in my other um here  I have another Repository I think it's called  
[5:00:34] the python in which I implemented the dbm paper  but by using this algorithm here so if you are  
[5:00:40] interested in this version of the denoising you  can check my other repository here this one ddpm  
[5:00:47] and I also wanted to show you how The impainting  Works how the how the image to image and how the  
[5:00:55] text image works of course the possibilities are  Limitless it all depends on the powerfulness of  
[5:01:02] the model and how you use it and I hope you use  it in a clever way to build the amazing products  
[5:01:08] I also want to thank very much many repositories  that I have used as a self-studding material so  
[5:01:15] because of course I didn't make up all this by  myself I studied a lot of papers I read I think  
[5:01:20] to study these division models I read more than  30 papers in the last few weeks so it took me a  
[5:01:27] lot of time but I was really passionate about this  kind of models because they're complicated and I  
[5:01:32] really like to study things that can generate new  stuff so I want to really thank uh in particularly  
[5:01:39] some resources that I have used let me see these  ones here so the official code the this guy Diva  
[5:01:47] Gupta this other repository from um this person  here which I used very much actually as a base  
[5:01:54] and the diffusers library from this hugging  face upon which I based most of the code of  
[5:02:00] my sampler because I think it's better to  use because we are actually just applying  
[5:02:04] some formulas there is no point in writing it  from zero the point is actually understanding  
[5:02:08] what is happening with these formulas and why we  are doing it the things we are doing and as usual  
[5:02:14] the full code is available I will also make all  the slides available for you guys and I hope if  
[5:02:19] you are in China you also have a great holiday  with me and if you're not in China I hope you  
[5:02:23] have a great time with your family and friends  and everyone else so welcome back to my channel  
[5:02:28] anytime and please feel free to comment on send  me a comment or if you didn't understand something  
[5:02:34] or if you want me to explain something better  because I'm always available for explanation  
[5:02:39] and guys I do this not as my full-time job of  course I do it as a part-time and lately I'm  
[5:02:47] I'm doing consulting so I'm very busy but  sometimes I take time to record videos  
[5:02:52] and so please share my channel Share my video  with people if you like it and so that my kind  
[5:02:58] channel can grow and I have more motivation to  keep doing this kind of videos which take really  
[5:03:02] a lot of time because to prepare a video like this  I spend around many weeks of research but this is  
[5:03:09] okay I do it for as a passion I don't do it as a  job and I spend a really a lot of time preparing  
[5:03:15] all the slides and preparing all the speeches  and preparing and the code and cleaning it and  
[5:03:20] commending it etc etc I always do it for free  so if you would like to support me the best way  
[5:03:26] is to subscribe like my video and share it with  other people thank you guys and have a nice day
